{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# What does 1GRDvs64 SIR_HIST_SD do\n",
    "\n",
    "## Things we should do with this or a new related notebook\n",
    "\n",
    "** Note that the calculation of the spatial standard deviation is ONLY done in this notebook, not the map, but if we rewrite, we should make it so that the area that can be calculated is flexible (the GRD/64SIR extent, the 8 around the SIR pixel of interest)\n",
    "\n",
    "** clean up notebooks to allow for separately doing (some of) Separate time series, multi plots, histograms, export of data , test breaks related to if you haven't run the prior cells]\n",
    "\n",
    "** Use standardized filenames for pkl/tif files\n",
    "\n",
    "** If we are reading in the whole cube, then could we change the points after the read in (but before the DAV, SD calcs, or just different pixel ids) so that we could more efficiently process subsets \n",
    "\n",
    "** Some of the original maps don't work and need updating. I think they are kind of useful for quick looks,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things we should do with this or a new related notebook\n",
    "\n",
    "**Build this to use the pkl files from Large_Melt_Map. This would allow less processing time and less duplication. It would also allow us to do these things for more pixels and areas at a time, which would increase the exploration piece. \n",
    "\n",
    "\n",
    "\n",
    "**Note that the calculation of the spatial standard deviation is ONLY done in this notebook, not the map, but if we rewrite, we should make it so that the area that can be calculated is flexible (the GRD/64SIR extent, the 8 around the SIR pixel of interest)\n",
    "\n",
    "\n",
    "\n",
    "**clean up notebooks to allow for separately doing (some of) Separate time series, multi plots, histograms, export of data , test breaks related to if you haven't run the prior cells]\n",
    "\n",
    "**It would be good to standardize the filenames for export so that the sitename goes into the filename so we don't overwrite and mix up the output; MJ added threshold, count, window, cubesubset to filenames in large meltmap\n",
    "e.g. have output depend on the pkl file input (add a label, hist etc)\n",
    "new workflow could be to start with the pkl file/cube/site e.g.\n",
    "also think about what we can read in bcs this is working w raw data (TB), SD, SSD, DAV etc.\n",
    "\n",
    "**If we are reading in the whole cube, then could we change the points after the read in (but before the DAV, SD calcs, or just different pixel ids) so that we could more efficiently process subsets\n",
    "**Some of the original maps don't work and need updating. I think they are kind of useful for quick looks, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from netCDF4 import Dataset, num2date\n",
    "import numpy as np\n",
    "import os\n",
    "if os.name == 'nt':\n",
    "    os.environ[\"PROJ_LIB\"] = os.path.join(os.environ[\"CONDA_PREFIX\"], \"Library\", \"share\")\n",
    "    os.environ[\"GDAL_DATA\"] = os.path.join(os.environ[\"CONDA_PREFIX\"], \"Library\", \"share\", \"gdal\")\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "from cetbtools.ease2conv import Ease2Transform\n",
    "\n",
    "# the set_trace() command is helpful for entering interactive\n",
    "# debugging mode to step through lines of code in this notebook\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas display options to display entire DataFrame, use None for all \n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.min_rows', 120)\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.width', 120)\n",
    "pd.set_option('display.max_colwidth', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the local machine location of CETB data cubes\n",
    "# This directory is expected to contain subdirectories in the following hierarchy\n",
    "# that duplicates the hierarchy on the Google Shared Drive NSIDC-SD-CETB/v1/, \n",
    "# for example:\n",
    "# dataDir/F13_SSMI/N/nc_cubes/cubes_<regionName>\n",
    "\n",
    "#JR notes setting up for Lunga, not sure if this is most recent version\n",
    "\n",
    "user = 'MAHMac' #Mahboubeh #Mariah #MJWindows\n",
    "if ('Joan' == user):\n",
    "    dataDir = '/home/jmr204/rdrive/jmr204group/CETB_cubes/' #jmr lunga #'/mnt/data3/cetb/nsidc0630_v1/' #jmr machine fringe \n",
    "    scriptDir = '/home/jmr204/ipynb_melt_onset/scripts'#lunga #'/mnt/data3/cetb/ipynb_melt_onset/scripts' #fringe\n",
    "    outDir = '/home/jmr204/cetb/ipynb_melt_onset_plots'#'/mnt/data3/cetb/ipynb_melt_onset_plots'\n",
    "    outNoisy = Path(Path.home(), 'noisy')\n",
    "elif ('Mariah' == user):\n",
    "    dataDir = Path(Path.home(), 'nsidc0630_v1') # Mariah's PC or Mary Jo's Mac\n",
    "    scriptDir = Path(Path.home(), 'ipynb_melt_onset', 'scripts')\n",
    "    outDir = Path(Path.home(), 'ipynb_melt_onset_plots') \n",
    "    outNoisy = Path(Path.home(), 'noisy')\n",
    "elif ('MJWindows' == user):\n",
    "    dataDir = Path('Z:/mj On My Mac/nsidc0630_v1') # Mary Jo's Windows machine\n",
    "    scriptDir = Path(Path.home(), 'ipynb_melt_onset', 'scripts')\n",
    "    outDir = Path(Path.home(), 'ipynb_melt_onset_plots')\n",
    "    outNoisy = Path(Path.home(), 'noisy')\n",
    "elif ('MJMac' == user):\n",
    "    dataDir = Path(Path.home(), 'nsidc0630_v1') # Mary Jo's Mac\n",
    "    scriptDir = Path(Path.home(), 'ipynb_melt_onset', 'scripts')  \n",
    "    outDir = Path(Path.home(), 'nsidc0630_v1') \n",
    "    outNoisy = Path(Path.home(), 'nsidc0630_v1', 'noisy')\n",
    "elif ('MAHMac' == user):\n",
    "    dataDir = Path(Path.home(), 'nsidc0630_v1') # Mary Jo's Mac\n",
    "    scriptDir = Path(Path.home(), 'Projects', 'ipynb_melt_onset', 'scripts')  \n",
    "    outDir = Path(Path.home(), 'nsidc0630_v1') \n",
    "    outNoisy = Path(Path.home(), 'nsidc0630_v1', 'noisy')\n",
    "elif ('Mahboubeh' == user):\n",
    "    dataDir = Path('Z:/mj On My Mac/nsidc0630_v1') # Mary Jo's Windows machine\n",
    "    scriptDir = Path(Path.home(), 'ipynb_melt_onset', 'scripts')\n",
    "    outDir = Path(Path.home(), 'ipynb_melt_onset_plots')\n",
    "    outNoisy = Path(Path.home(), 'noisy')\n",
    "else:\n",
    "    raise ValueError(\"unknown user= %s\\n\" % (user) )\n",
    "    \n",
    "# Make the output directory for plot images, if it doesn't yet exist\n",
    "if not os.path.exists(outDir):\n",
    "    os.makedirs(outDir)\n",
    "    \n",
    "if not os.path.exists(outNoisy):\n",
    "    os.makedirs(outNoisy)\n",
    "    \n",
    "noisydirs = ['sd', 'noisytb_csv', 'noisydav_csv', 'noisytbSIR_csv', 'noisydavSIR_csv']\n",
    "for dir in noisydirs:\n",
    "    nextDir = Path(outNoisy, dir)\n",
    "    if not os.path.exists(nextDir):\n",
    "        os.makedirs(nextDir)\n",
    "    \n",
    "%cd $scriptDir\n",
    "dataDir, user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisydirs = ['sd', 'noisytb_csv', 'noisydav_csv', 'noisytbSIR_csv', 'noitydavSIR_csv']\n",
    "for dir in noisydirs:\n",
    "    print('dir=%s' % dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from CETB_IO import read_Tb\n",
    "from CETB_IO import read_Tb_std_dev\n",
    "from CETB_IO import coords\n",
    "from CETB_IO import read_Tb_whole\n",
    "from CETB_IO import find_cube_offset\n",
    "from CETB_IO import grid_locations_of_subset\n",
    "from CETB_IO import years_for\n",
    "from CETB_IO import get_sir_info\n",
    "from CETB_IO import get_site_boundaries\n",
    "from CETB_algorithms import DAV_MOD\n",
    "from CETB_algorithms import XPGR\n",
    "from CETB_algorithms import calc_DAV\n",
    "from CETB_algorithms import end_high_DAV\n",
    "from CETB_algorithms import D_DAV\n",
    "from CETB_algorithms import Winter_DAV\n",
    "from CETB_analysis import Tb_hist_annual\n",
    "from CETB_analysis import DAV_hist_annual\n",
    "from CETB_analysis import Tb_hist_monthly\n",
    "from CETB_analysis import DAV_hist_monthly\n",
    "from CETB_analysis import TbDAV_series_one_year\n",
    "from CETB_analysis import early_melt_events\n",
    "from CETB_analysis import min_max_series\n",
    "from CETB_analysis import MOD_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_site_boundaries('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify region, satellite, sensor, channel, and image reconstruction algorithm of interest in file name\n",
    "# this notebook will read in 2 CETB datasets so that channels/algorithms/sensors can be compared\n",
    "region='AKYukon'  #make this the same syntax as cubefilenames and sub-directory #GLaIL #AKYukon CEurope\n",
    "sat_GRD='F18'   #'GCOMW1' #'AQUA' for AMSRE, 'F13','F14','F15'... for SSMI\n",
    "sat_SIR= 'F18'\n",
    "sensor_GRD='SSMIS'  #'AMSRE', 'SSMI', 'SSMIS' 'AMSR2' etc.\n",
    "sensor_SIR='SSMIS'\n",
    "channel_GRD='37V'  #'36V','36H', '18V','18H', etc. '19V','19H' and '37V','37H' for SSMI)\n",
    "channel_SIR='37V'\n",
    "alg_GRD='GRD'   #SIR or GRD\n",
    "alg_SIR='SIR'\n",
    "\n",
    "hemName = 'N'   \n",
    "\n",
    "# get sir to grd factor and sir_gpd name\n",
    "sir_2_grd_factor, sir_gpd = get_sir_info(channel_SIR, hem=hemName)\n",
    "\n",
    "cubeType_GRD = channel_GRD + '-' + alg_GRD\n",
    "cubeType_SIR = channel_SIR + '-' + alg_SIR\n",
    "  \n",
    "if ('SSMI' == sensor_GRD) or ('SSMIS' == sensor_GRD):\n",
    "    provider='CSU' \n",
    "    version='v1.*'\n",
    "elif ('AMSR2' ==sensor_GRD):\n",
    "    provider='PPS_XCAL' #need to check this\n",
    "    version='v1.1'\n",
    "elif 'AMSRE' == sensor_GRD:\n",
    "    provider='RSS'\n",
    "    version='v1.3'\n",
    "\n",
    "Years_all=years_for(sat_GRD)\n",
    "#might want to truncate Years to subset if very slow during testing\n",
    "#if we give it more years than available what do we want it to do? \n",
    "#warn me but return what it finds\n",
    "    \n",
    "# either change to the directory where the data is or specify it\n",
    "datadir_GRD = \"%s/%s_%s/%s/nc_cubes/cubes_%s\" % (\n",
    "    dataDir, sat_GRD, sensor_GRD, hemName, region)\n",
    "datadir_SIR = \"%s/%s_%s/%s/nc_cubes/cubes_%s\" % (\n",
    "    dataDir, sat_SIR, sensor_SIR, hemName, region)\n",
    "\n",
    "\n",
    "# filepath patterns for each type of cubefile\n",
    "prefix_GRD = \"CETB.cubefile.%s.%s_%s-%s-%s-%s-%s\" % (\n",
    "    region, sat_GRD, sensor_GRD, channel_GRD, alg_GRD, provider, version)\n",
    "prefix_SIR = \"CETB.cubefile.%s.%s_%s-%s-%s-%s-%s\" % (\n",
    "    region, sat_SIR, sensor_SIR, channel_SIR, alg_SIR, provider, version)\n",
    "\n",
    "#Get lat/lon boundaries and SiteLabel for plots from get_site_boundaries\n",
    "#Enter a label with no punctuation or spaces for filenames\n",
    "\n",
    "#Site='114 km South of Barrow airport, AK' #114 km south of airport to get an inland site\n",
    "SiteLabel='Barrow114kmS'\n",
    "#SiteLabel='NEUkrain'\n",
    "lat_start, lat_end, lon_start, lon_end, Site=get_site_boundaries(SiteLabel)\n",
    "lat_start, lat_end, lon_start, lon_end, Site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the GRD filename prefix is really what we have on the local machine\n",
    "datadir_GRD, prefix_GRD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get row/col coordinates of cells to examine\n",
    "\n",
    "Find the N25 (row, col) coordinates that correspond to our site coordinates\n",
    "\n",
    "Convert this single GRD cell coordinate to the envelope of SIR cell (row, col) coordinates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rows_cols_GRD = coords(datadir_GRD, prefix_GRD, lat_start, lat_start, lon_start, lon_start)\n",
    "rows_cols_env = tuple(np.array(rows_cols_GRD) * sir_2_grd_factor)\n",
    "print(rows_cols_GRD)\n",
    "print(rows_cols_env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir_SIR, prefix_SIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the string that is the index to the SIR cell closest to our site\n",
    "# note that this is likely inside the SIR cell envelope\n",
    "rows_cols = coords(datadir_SIR, prefix_SIR, lat_start, lat_start, lon_start, lon_start)\n",
    "Site_nearest_row_col = \"%s,%s\" % (str(rows_cols[0]),str(rows_cols[2])) \n",
    "Site_nearest_row_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the directory, filename pattern and Years are what we expect\n",
    "# subset Years to a shorter set if only testing, because this takes a long time\n",
    "subYears = Years_all[:2]\n",
    "datadir_SIR, prefix_SIR, subYears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new cell to read whole dataset\n",
    "#takes a long time, for testing substitute a year for Years\n",
    "# See implementation of read_Tb_whole in the scripts directory for what is passed back\n",
    "data_SIR = read_Tb_whole(datadir_SIR, prefix_SIR, subYears,\n",
    "                         rows_cols_env[0], rows_cols_env[1], rows_cols_env[2], rows_cols_env[3])\n",
    "\n",
    "data_GRD = read_Tb_whole(datadir_GRD, prefix_GRD, subYears, \n",
    "                         rows_cols_GRD[0], rows_cols_GRD[1], rows_cols_GRD[2], rows_cols_GRD[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_SIR['latitude'].data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate DAV for the Tb data that was imported\n",
    "DAV_GRD = calc_DAV(data_GRD['TB'])\n",
    "DAV_SIR = calc_DAV(data_SIR['TB'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New output from MOD_array\n",
    "\n",
    "MOD_DOY_array: this is a masked array (used to be the only thing Mitch returned), contains doy of average melt over all years for each pixel. It is now a single column in MOD_DOY_df.\n",
    "\n",
    "MOD_DOY_df: MJB: I think this is more useful, it is a data frame with 1 row for each pixel, and 1 column for each year, plus extra columns for the 'Avg' 'Max' and 'Min'. Contents of the data are doy of first time melt criteria were met. 'Avg' column is the contents of the old MOD_DOY_array. There are also columns with geolocation for each pixel x,y, row, col, lat, lon\n",
    "\n",
    "meltflag_df: This may also be useful for further analysis, 1 row for each date (2 dates per day), 1 column for each pixel. Values are 1 if DAV/TB thresholds were exceeded on this date, 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up MOD parameters\n",
    "# can tweak thresholds and count/window, legacy algorithm used \n",
    "# 3 occurrences in 5 day window and shows '3' (count) with the first pixel in the subset to melt because of the counter\n",
    "#3 out of5 days is common for lower latitudes or intermittent melt (But not too intermittent)\n",
    "\n",
    "# From Matias Fall 2022 forward facing indexer to assign rolling sum value to the beginning of window\n",
    "# Setting the window_size sets the number of observations,'14' would be 7 days (2 measurements per day)\n",
    "# If you don't want to use the forward facing indexer, then change \"window\" to a numeral, this will assign\n",
    "# the rolling sum value to the end of the window\n",
    "MOD_window = 1 # for 5 days, change this to 10\n",
    "MOD_count = 1 # for 3 occurrences change this to 3\n",
    "indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=MOD_window)\n",
    "window = indexer\n",
    "\n",
    "EHD_window = 1\n",
    "EHD_count = 1\n",
    "if ('SSMI' == sensor_GRD) or ('SSMIS' == sensor_GRD):\n",
    "    DAV_threshold=10 \n",
    "    Tb_threshold=247\n",
    "elif 'AMSRE' == sensor_GRD:\n",
    "    DAV_threshold=18 \n",
    "    Tb_threshold=252\n",
    "elif 'AMSR2' == sensor_GRD:\n",
    "    DAV_threshold=18 \n",
    "    Tb_threshold=252"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get array of MODs for each pixel in the 64 set to use for map\n",
    "# This will contain each year and the Avg MOD for all years\n",
    "# calculate melt onset date, each row returned is the MOD for that year, \n",
    "MOD_DOY_df, meltflag_df, EHD_DOY_df, EHDflag_df = MOD_array(\n",
    "    datadir_SIR, prefix_SIR, data_SIR, DAV_SIR, rows_cols_env, \n",
    "    subYears, window, MOD_count, EHD_window, EHD_count, DAV_threshold, Tb_threshold)\n",
    "MOD_DOY_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOD of the GRD pixel \n",
    "# Use the same thresholds/criteria as for SIR\n",
    "MOD_DOY_GRD_df, meltflag_GRD_df, EHD_DOY_GRD_df, EHDflag_GRD_df = MOD_array(\n",
    "    datadir_GRD, prefix_GRD, data_GRD, DAV_GRD, rows_cols_GRD, \n",
    "    subYears, window, MOD_count, EHD_window, EHD_count, DAV_threshold, Tb_threshold)\n",
    "MOD_DOY_GRD_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trial-and-error decide on size for GRD vs. SIR scatter markers\n",
    "#np.sqrt((1200)/8)^2\n",
    "sSIR = 100\n",
    "sGRD = 3200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outFile = Path(outDir, \"%s.png\" % SiteLabel)\n",
    "\n",
    "# PLOT the MOD MAP - avg of all years or one year of interest\n",
    "# Decide here:\n",
    "#plot_type='all' # DO THIS to show avg of all years\n",
    "# OR\n",
    "plot_type='year' # DO THIS and choose a year for 1 year of interest\n",
    "year=2010\n",
    "## fixme ## the scale bar does not work for single year (values range -.1 -+.1)\n",
    "\n",
    "if plot_type == 'all':\n",
    "    SIR_array = MOD_DOY_df['Avg'].values\n",
    "    GRD_array = MOD_DOY_GRD_df['Avg'].values\n",
    "    label = 'Avg DOY ' + str(Years[0]) + '-' + str(Years[-1])\n",
    "    title = sensor_SIR + '-' + Site + ' - MOD - (' + label + ')'\n",
    "elif plot_type == 'year':\n",
    "    SIR_array = MOD_DOY_df[year].values\n",
    "    GRD_array = MOD_DOY_GRD_df[year].values\n",
    "    label = str(year) + 'DOY' \n",
    "    title = sensor_SIR + '-' + Site + ' - MOD - (' + label + ')'\n",
    "\n",
    "# Set a few common things\n",
    "#min day of year to plot--look at both data arrays and subtract 2 days\n",
    "minday = np.amin([np.amin(GRD_array), np.amin(SIR_array)]) - 2\n",
    "#max day of year to plot same here but add 2 days\n",
    "maxday = np.amax([np.amax(GRD_array), np.amax(SIR_array)]) + 2\n",
    "\n",
    "graticule_fontsize = 4\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10,8))\n",
    "\n",
    "axes[0].set_title(title)\n",
    "# create polar stereographic Basemap instance.\n",
    "m = Basemap(ax=axes[0], projection='stere',lon_0=lon_start,lat_0=lat_start,lat_ts=90.,\\\n",
    "            llcrnrlat=(lat_start-.7),urcrnrlat=(lat_start+.7),\\\n",
    "            llcrnrlon=(lon_start-.7),urcrnrlon=(lon_start+.7),\\\n",
    "            rsphere=6371200.,resolution='l',area_thresh=10000, epsg=3857)\n",
    "m.drawcoastlines()\n",
    "m.drawcountries()\n",
    "m.drawstates()\n",
    "#m.etopo(scale=5, alpha=0.7)\n",
    "#m.bluemarble(scale=1)\n",
    "parallels = np.arange(0.,90,0.5)\n",
    "m.drawparallels(parallels,labels=[1,0,0,0],fontsize=10)\n",
    "# draw meridians\n",
    "meridians = np.arange(180.,360.,0.5)\n",
    "m.drawmeridians(meridians,labels=[0,0,0,1],fontsize=10)\n",
    "m.arcgisimage(service='World_Physical_Map', xpixels = 1500, verbose= True)\n",
    "\n",
    "x,y = m(MOD_DOY_df['longitude'].values, MOD_DOY_df['latitude'].values)\n",
    "#m.plot(x,y, 'bs', markersize=11, markeredgewidth=1, alpha=0.2)\n",
    "im0 = m.scatter(x, y, c=SIR_array, s=sSIR, marker='s', lw=0, cmap='BuPu_r', alpha=.8,\n",
    "                vmin=minday, vmax=maxday)\n",
    "\n",
    "# Get the current axes and configure placement so colorbar will\n",
    "# be 5% of ax width and the padding will be 0.08 in.\n",
    "divider = make_axes_locatable(axes[0])\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.08)\n",
    "cbar = plt.colorbar(im0, cax=cax, label='DOY')\n",
    "\n",
    "# plot a red line on colorbar for the MOD for the GRD pixel\n",
    "cbar.ax.hlines(cbar.norm(GRD_array[0]), 0, 1, color='green') \n",
    "\n",
    "# create polar stereographic Basemap instance.\n",
    "m = Basemap(ax=axes[1], projection='stere',lon_0=lon_start,lat_0=lat_start,lat_ts=90.,\\\n",
    "            llcrnrlat=(lat_start-.7),urcrnrlat=(lat_start+.7),\\\n",
    "            llcrnrlon=(lon_start-.7),urcrnrlon=(lon_start+.7),\\\n",
    "            rsphere=6371200.,resolution='l',area_thresh=10000, epsg=3857)\n",
    "m.drawcoastlines()\n",
    "m.drawcountries()\n",
    "m.drawstates()\n",
    "#m.etopo(scale=5, alpha=0.7)\n",
    "#m.bluemarble(scale=1)\n",
    "parallels = np.arange(0.,90,0.5)\n",
    "m.drawparallels(parallels,labels=[1,0,0,0],fontsize=10)\n",
    "# draw meridians\n",
    "meridians = np.arange(180.,360.,0.5)\n",
    "m.drawmeridians(meridians,labels=[0,0,0,1],fontsize=10)\n",
    "m.arcgisimage(service='World_Physical_Map', xpixels = 1500, verbose= True)\n",
    "\n",
    "x,y = m(MOD_DOY_GRD_df['longitude'].values, MOD_DOY_GRD_df['latitude'].values)\n",
    "#m.plot(x,y, 'bs', markersize=11, markeredgewidth=1, alpha=0.2)\n",
    "im1 = m.scatter(x, y, c=GRD_array, s=sGRD, marker='s', lw=0, cmap='BuPu_r', alpha=.8,\n",
    "                vmin=minday, vmax=maxday)          \n",
    "\n",
    "# Get the current axes and configure placement so colorbar will\n",
    "# be 5% of ax width and the padding will be 0.05 in.\n",
    "divider = make_axes_locatable(axes[1])\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "cbar = plt.colorbar(im1, cax=cax, label='DOY')\n",
    "\n",
    "plt.tight_layout(pad=3.0)\n",
    "plt.savefig(outFile)\n",
    "plt.show()\n",
    "\n",
    "print(\"figure saved to %s\\n\" % outFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load SIR Tb standard deviation data from netCDF file with function\n",
    "Tb_std_dev=read_Tb_std_dev(datadir_SIR, prefix_SIR, subYears,\n",
    "                           rows_cols_env[0],rows_cols_env[1],rows_cols_env[2],rows_cols_env[3])\n",
    "#Tb_std_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#need to run this cell for the max DAV to work\n",
    "y_dims_list=list(range(len(data_SIR['TB'][0,:,0])))  # creates a list of the y-dimension pixel indices, used for plotting\n",
    "x_dims_list=list(range(len(data_SIR['TB'][0,0,:])))\t# creates a list of the x-dimension pixel indices, for plotting\n",
    "y_s=list(range(rows_cols_env[0],rows_cols_env[1]))  # makes a list of the y(row) numbers so the dataframe of early melt events can be indexed\n",
    "x_s=list(range(rows_cols_env[2],rows_cols_env[3]))  # makes a list of the x(col) numbers so the dataframe of early melt events can be indexed\n",
    "\n",
    "# create dataframe of the 64 SIR pixels\n",
    "Tb_std_frame=pd.DataFrame()\n",
    "for i in y_dims_list:\n",
    "    for j in x_dims_list:\n",
    "        column=pd.DataFrame(data=Tb_std_dev[:,i,j], columns=[str(y_s[i])+','+str(x_s[j])])\n",
    "        Tb_std_frame=pd.concat([Tb_std_frame,column],axis=1)\n",
    "Tb_std_frame.index=data_SIR['cal_date']\n",
    "Tb_std_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframe of Tb GRD for plotting Tb on specified day and observation\n",
    "Tb_std_dev_GRD = read_Tb_std_dev(datadir_GRD, prefix_GRD, subYears,\n",
    "                                 rows_cols_GRD[0],rows_cols_GRD[1],rows_cols_GRD[2],rows_cols_GRD[3])\n",
    "Tb_std_dev_GRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# slice of dataframe described above\n",
    "Tb_sd_frame_GRD=pd.DataFrame(Tb_std_dev_GRD[:,0,0], index=data_GRD['cal_date'])\n",
    "Tb_sd_frame_GRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframe of Tb GRD for plotting Tb on specified day and observation\n",
    "Tb_frame_GRD = pd.DataFrame(data_GRD['TB'][:,0,0], index=data_GRD['cal_date'])\n",
    "Tb_frame_GRD.columns=['TB']\n",
    "Tb_frame_GRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframe of Tb SIR for plotting Tb on specified day and observation\n",
    "Tb_frame=pd.DataFrame()\n",
    "for i in y_dims_list:\n",
    "    for j in x_dims_list:\n",
    "        column=pd.DataFrame(data=data_SIR['TB'][:,i,j], columns=[str(y_s[i])+','+str(x_s[j])])\n",
    "        Tb_frame=pd.concat([Tb_frame,column],axis=1)\n",
    "Tb_frame.index=data_SIR['cal_date']\n",
    "Tb_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the min and max Tb of the 64 for each time step\n",
    "Tb_64_min=Tb_frame.min(axis=1)\n",
    "Tb_64_max=Tb_frame.max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute std dev of the 64 SIR pixels\n",
    "Tb_std_64=Tb_frame.std(axis=1)\n",
    "Tb_std_64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take difference of the GRD_std_dev variable and the computed sd of the 64 SIR pixels\n",
    "Tb_sd_diff=Tb_sd_frame_GRD[0]-Tb_std_64\n",
    "Tb_sd_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAV_GRD_frame=pd.DataFrame(DAV_GRD[:,0,0], index=data_GRD['cal_date'])\n",
    "DAV_GRD_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DAV_SIR_frame=pd.DataFrame()\n",
    "for i in y_dims_list:\n",
    "    for j in x_dims_list:\n",
    "        column=pd.DataFrame(data=DAV_SIR[:,i,j], columns=[str(y_s[i])+','+str(x_s[j])])\n",
    "        DAV_SIR_frame=pd.concat([DAV_SIR_frame,column],axis=1)\n",
    "DAV_SIR_frame['Date']=data_SIR['cal_date']        \n",
    "DAV_SIR_frame=DAV_SIR_frame.set_index('Date')\n",
    "DAV_SIR_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAV_nearest=DAV_SIR_frame[Site_nearest_row_col]\n",
    "DAV_nearest=DAV_nearest.to_frame()\n",
    "DAV_nearest.columns=['DAV']\n",
    "DAV_nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tb_nearest=Tb_frame[Site_nearest_row_col]\n",
    "Tb_nearest=Tb_nearest.to_frame()\n",
    "Tb_nearest.columns=['TB']\n",
    "Tb_nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Site_nearest_row_col, sat_SIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outFile = Path(outDir, '%s_%s_%s-%sTS.png' % (sat_SIR, SiteLabel, str(subYears[0]), str(subYears[-1])))\n",
    " \n",
    "#Generate a time series of specified length (any window in dataset)\n",
    "#Make sure to update filename in first line to export new file\n",
    "fig,(ax1,ax2,ax3)=plt.subplots(3,1, sharex=True, figsize=(10,10))\n",
    "\n",
    "ax1.set_title(str(lat_start)+'_'+str(lon_start)+' '+Site)\n",
    "#Tb_sd_frame_GRD.plot(xlim=['2019-01-01','2019-12-31'])\n",
    "\n",
    "ax1.plot(Tb_64_max, color='red', label='SIR-Tb-max')\n",
    "ax1.plot(Tb_64_min, color='blue', label='SIR-Tb-min')\n",
    "ax1.plot(Tb_nearest, color = 'black',label='SIR-Site')\n",
    "ax1.plot(Tb_frame_GRD, color='#636363', label='GRD-Tb')\n",
    "ax1.set_ylabel('Tb (K)', fontsize=16)\n",
    "ax1.set_ylim(200,300)\n",
    "#ax1.yticks(fontsize=12)\n",
    "ax1.legend(fontsize=12)\n",
    "ax2.plot(Tb_sd_frame_GRD, color='black', label='GRD-StDev')\n",
    "ax2.plot(Tb_std_frame[Site_nearest_row_col], color='#fdbb84', label='SIR-1px')\n",
    "ax2.set_ylabel('Tb-STD (K)',fontsize=16)\n",
    "ax2.plot(Tb_std_64, color='#4292c6',label='SIR-64px-StDev')\n",
    "ax2.plot(Tb_sd_diff, label='StDevDiff_GRD-64SIR', color='green')\n",
    "ax2.legend(fontsize=10,loc='lower right')\n",
    "ax2.set_ylim(-12,30)\n",
    "ax3.plot(DAV_GRD_frame, color='black', label='GRD-DAV')\n",
    "ax3.plot(DAV_SIR_frame[Site_nearest_row_col], color='#4292c6', label='SIR-DAV')\n",
    "ax3.set_ylabel('DAV (K)',fontsize=16)\n",
    "ax3.legend(fontsize=12)\n",
    "ax3.set_ylim(0,50)\n",
    "#ax3.set_xlim('2022-01-01', '2022-10-31') #\n",
    "\n",
    "#ax1.set_xlim('2010-01-01', '2011-12-31') #there is something wrong with this line (new since 10-26-21) \n",
    "#fix above line probably changing date format\n",
    "#related to matplotlib version maybe\n",
    "fig.autofmt_xdate()\n",
    "plt.savefig(outFile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outFile = Path(outDir, '%s-SSMIstackeddav%s-%s.png' % (SiteLabel, str(subYears[0]), str(subYears[-1])))\n",
    "\n",
    "#time range default Jan 1 - Dec 31, 1 year\n",
    "#subset of a year is a shorter value\n",
    "startmonth= 1\n",
    "startday=1\n",
    "endmonth=12\n",
    "endday=31\n",
    "\n",
    "#generate a plot with stacked DAV\n",
    "fig,ax=plt.subplots(len(subYears),1, figsize=(10,14))\n",
    "fig.suptitle(str(lat_start)+'_'+str(lon_start)+' '+Site, fontweight='bold')\n",
    "\n",
    "for i, (yr, df) in enumerate(DAV_SIR_frame.groupby(DAV_SIR_frame.index.year)):\n",
    "    #print(i,yr)\n",
    "    \n",
    "    df.plot(ax=ax[i],y=[Site_nearest_row_col], color='#4292c6')#, #label=str(yr)+' SIR-DAV')\n",
    "\n",
    "    \n",
    "    startdt=pd.to_datetime('%04d%02d%02d' % (yr, startmonth, startday), format='%Y%m%d')\n",
    "    enddt=pd.to_datetime('%04d%02d%02d' % (yr, endmonth, endday), format='%Y%m%d')\n",
    "    \n",
    "    ax[i].set_ylabel('DAV (K)')#fontsize=16, #fontweight='bold')\n",
    "    ax[i].set_ylim(0,50)\n",
    "    ax[i].axhline(y=DAV_threshold, color='gray', linestyle='dashed')\n",
    "    #ax.tick_params(right= True,top= True,left= True, bottom= True)\n",
    "    #ax[i].set_xlabel('month')\n",
    "    #ax[i].tick_params(labelbottom=False)\n",
    "    ax[i].set_xlim(startdt, enddt)\n",
    "    #ax[i].legend(fontsize=12)\n",
    "    \n",
    "fig.tight_layout()\n",
    "    \n",
    "plt.savefig(outFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tb_sd_frame_GRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#working\n",
    "#I made this work for the GRD SD but what I really want to do is create\n",
    "# a secondary axis and show DAV and SD on different axes\n",
    "#attempts are commented out because I obviously don't totally understand the loop\n",
    "\n",
    "outFile = Path(outDir, '%s-SSMIstackedDAV+SD%s-%s.png' % (SiteLabel, str(subYears[0]), str(subYears[-1])))\n",
    "\n",
    "#generate a plot with stacked DAV\n",
    "fig,ax=plt.subplots(len(subYears),1, figsize=(10,14))\n",
    "\n",
    "fig.suptitle(str(lat_start)+'_'+str(lon_start)+' '+Site, fontweight='bold')\n",
    "\n",
    "for i, (yr, df) in enumerate(DAV_SIR_frame.groupby(DAV_SIR_frame.index.year)):\n",
    "\n",
    "    df.plot(ax=ax[i],y=[Site_nearest_row_col], color='#4292c6')\n",
    "    ax[i].legend([str(yr)+' SIR-DAV'], loc='upper left')\n",
    "    ax[i].set_ylabel('SIR DAV (K)')\n",
    "    ax[i].set_ylim(0,30)\n",
    "    ax[i].axhline(y=DAV_threshold, color='gray', linestyle='dashed')\n",
    "    \n",
    "    ax_right = ax[i].twinx()\n",
    "    Tb_sd_GRD_year = Tb_sd_frame_GRD[Tb_sd_frame_GRD.index.year == yr]\n",
    "    Tb_sd_GRD_year.plot(ax=ax_right, color='black')\n",
    "    ax_right.legend([str(yr)+' GRD StDev'], loc='upper right')\n",
    "    ax_right.set_ylabel('GRD StDev (K)')#fontsize=16, #fontweight='bold')\n",
    "    ax_right.set_ylim(0,10)\n",
    "        \n",
    "fig.tight_layout()\n",
    "    \n",
    "plt.savefig(outFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outFile = Path(outDir, '%s-stackedtb2.png' % SiteLabel)\n",
    "\n",
    "#generate a plot with stacked DAV\n",
    "fig,ax=plt.subplots(len(subYears),1, figsize=(10,14))\n",
    "fig.suptitle(str(lat_start)+'_'+str(lon_start)+' '+Site, fontweight='bold')\n",
    "\n",
    "for i, (yr, df) in enumerate(DAV_nearest.groupby(DAV_nearest.index.year)):\n",
    "\n",
    "    # Subset each dataframe for just this year\n",
    "\n",
    "    Tb_64_max_year = Tb_64_max[Tb_64_max.index.year == yr]\n",
    "    Tb_64_min_year = Tb_64_min[Tb_64_min.index.year == yr]\n",
    "    Tb_GRD_year = Tb_frame_GRD[Tb_frame_GRD.index.year == yr]\n",
    "\n",
    "    Tb_64_max_year.plot(ax=ax[i], color='red', label='SIR-Tb-max')\n",
    "    Tb_64_min_year.plot(ax=ax[i], color='blue', label='SIR-Tb-min')\n",
    "    Tb_GRD_year.plot(ax=ax[i], color='green', label='GRD-Tb')\n",
    "    \n",
    "    ax[i].set_ylabel('Tb (K)')#fontsize=16, #fontweight='bold')\n",
    "    ax[i].set_ylim(150,300)\n",
    "    ax[i].axhline(y=Tb_threshold, color = 'gray', linestyle ='dashed', label='Tb Threshold')\n",
    "    ax[i].legend(loc='upper left')\n",
    "    \n",
    "    ax_right=ax[i].twinx()\n",
    "    df.plot(ax=ax_right, y=['DAV'], color='black', label=['SIR-Site'])\n",
    "    ax_right.set_ylabel('DAV (K)')#fontsize=16, #fontweight='bold')\n",
    "    ax_right.set_ylim(0,70)\n",
    "    ax_right.axhline(y=DAV_threshold, color='red', linestyle='dashed', label='DAV Threshold')\n",
    "    ax_right.legend(loc='upper right')\n",
    "        \n",
    "fig.tight_layout()\n",
    "    \n",
    "plt.savefig(outFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outFile = Path(outDir, '%s_SDdif2.png' % SiteLabel)\n",
    "\n",
    "# histogram of the difference between GRD_Tb_std variable and the computed std of the 64 SIR pixels\n",
    "#plt.close()\n",
    "fig,ax=plt.subplots()\n",
    "values=Tb_sd_diff.dropna().values\n",
    "ax.hist(values,bins=500, label='STD_Dif GRD-64SIR')\n",
    "ax.set_title(Site+' all data')\n",
    "ax.axvline(x=Tb_sd_diff.mean(), color='red', linestyle='dashed', label=Tb_sd_diff.mean().round(3))\n",
    "ax.legend(fontsize=12)\n",
    "ax.set_xlim(-10,+10) #adding in specified limits JRM in progress\n",
    "ax.set_ylabel('Frequency',fontsize=14)\n",
    "ax.set_xlabel('Standard Deviation Difference (K)',fontsize=14)\n",
    "plt.savefig(outFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOD_DOY_df['Avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT the std dev of Tb MAP - \n",
    "date='2011-04-16' # specify date of interest\n",
    "obs=0 # 0 or 1 for morning or evening observation\n",
    "\n",
    "# load Tb_std_dev array\n",
    "array=Tb_std_frame.loc[date].values\n",
    "array=array[obs]\n",
    "GRD=Tb_sd_frame_GRD.loc[date].values\n",
    "GRD=GRD[obs]\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "# create figure and axes instances\n",
    "#fig = plt.figure()\n",
    "ax = fig.add_axes([0.1,0.1,0.8,0.8])\n",
    "#ax = fig.add_subplot(121)\n",
    "# create polar stereographic Basemap instance.\n",
    "m = Basemap(projection='stere',lon_0=lon_start,lat_0=lat_start,lat_ts=90.,\\\n",
    "            llcrnrlat=(lat_start-.7),urcrnrlat=(lat_start+.7),\\\n",
    "            llcrnrlon=(lon_start-.7),urcrnrlon=(lon_start+.7),\\\n",
    "            rsphere=6371200.,resolution='l',area_thresh=10000, epsg=3857)\n",
    "m.drawcoastlines()\n",
    "m.drawcountries()\n",
    "m.drawstates()\n",
    "#m.etopo(scale=5, alpha=0.7)\n",
    "#m.bluemarble(scale=1)\n",
    "parallels = np.arange(0.,90,0.5)\n",
    "m.drawparallels(parallels,labels=[1,0,0,0],fontsize=10)\n",
    "# draw meridians\n",
    "meridians = np.arange(180.,360.,0.5)\n",
    "m.drawmeridians(meridians,labels=[0,0,0,1],fontsize=10)\n",
    "m.arcgisimage(service='World_Physical_Map', xpixels = 1500, verbose= True)\n",
    "\n",
    "\n",
    "x,y=m(data_SIR['longitude'].data, data_SIR['latitude'].data)\n",
    "#m.plot(x,y, 'bs', markersize=11, markeredgewidth=1, alpha=0.2)\n",
    "m.scatter(x,y,c=array, s=100, marker='s',lw=0, cmap='Oranges', alpha=.8)  # cmap='Blues'\n",
    "x,y=m(data_GRD['longitude'].data, data_GRD['latitude'].data)\n",
    "m.plot(x,y,'ro',markersize=4,markeredgewidth=0.0)\n",
    "plt.title('Tb - std dev - '+date)\n",
    "\n",
    "# plot a red line on colorbar for the MOD for the GRD pixel\n",
    "cbar=plt.colorbar()\n",
    "cbar.ax.hlines(cbar.norm(GRD), 0, 1, color='green') \n",
    "#cbar.ax.clim(0,1)  #color bar limits\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_axes([0.1,0.1,0.8,0.8])\n",
    "\n",
    "# create polar stereographic Basemap instance.\n",
    "m = Basemap(projection='stere',lon_0=lon_start,lat_0=lat_start,lat_ts=90.,\\\n",
    "            llcrnrlat=(lat_start-.7),urcrnrlat=(lat_start+.7),\\\n",
    "            llcrnrlon=(lon_start-.7),urcrnrlon=(lon_start+.7),\\\n",
    "            rsphere=6371200.,resolution='l',area_thresh=10000, epsg=3857)\n",
    "m.drawcoastlines()\n",
    "m.drawcountries()\n",
    "m.drawstates()\n",
    "#m.etopo(scale=5, alpha=0.7)\n",
    "#m.bluemarble(scale=1)\n",
    "parallels = np.arange(0.,90,0.5)\n",
    "m.drawparallels(parallels,labels=[1,0,0,0],fontsize=10)\n",
    "# draw meridians\n",
    "meridians = np.arange(180.,360.,0.5)\n",
    "m.drawmeridians(meridians,labels=[0,0,0,1],fontsize=10)\n",
    "m.arcgisimage(service='World_Physical_Map', xpixels = 1500, verbose= True)\n",
    "\n",
    "x,y=m(data_SIR['longitude'].data, data_SIR['latitude'].data)\n",
    "#m.plot(x,y, 'bs', markersize=11, markeredgewidth=1, alpha=0.2)\n",
    "m.scatter(x,y,c=MOD_DOY_df['Avg'], s=150, marker='s',lw=0,cmap='Blues', alpha=.8)\n",
    "x,y=m(lon_start,lat_start)\n",
    "m.plot(x,y,'ro',markersize=4,markeredgewidth=0.0)\n",
    "plt.title('Tb - std dev')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#plots basemap but not info\n",
    "#probably needs updating if we want to keep it\n",
    "outFile = Path(outDir, 'test.png')\n",
    "\n",
    "# Plot Tb map\n",
    "#date='2019-05-08' # specify date of interest\n",
    "date = '2011-05-18'\n",
    "obs=0 # 0 for morning or 1 for evening observation\n",
    "\n",
    "# load Tb_std_dev array\n",
    "array=Tb_frame.loc[date].values\n",
    "array=array[obs]\n",
    "GRD=Tb_frame_GRD.loc[date].values\n",
    "GRD=GRD[obs]\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "# create figure and axes instances\n",
    "#fig = plt.figure()\n",
    "ax = fig.add_axes([0.1,0.1,0.8,0.8])\n",
    "#ax = fig.add_subplot(121)\n",
    "# create polar stereographic Basemap instance.\n",
    "m = Basemap(projection='stere',lon_0=lon_start,lat_0=lat_start,lat_ts=90.,\\\n",
    "            llcrnrlat=(lat_start-.7),urcrnrlat=(lat_start+.7),\\\n",
    "            llcrnrlon=(lon_start-.7),urcrnrlon=(lon_start+.7),\\\n",
    "            rsphere=6371200.,resolution='l',area_thresh=10000, epsg=3857)\n",
    "m.drawcoastlines()\n",
    "m.drawcountries()\n",
    "m.drawstates()\n",
    "#m.etopo(scale=5, alpha=0.7)\n",
    "#m.bluemarble(scale=1)\n",
    "parallels = np.arange(0.,90,0.5)\n",
    "m.drawparallels(parallels,labels=[1,0,0,0],fontsize=10)\n",
    "# draw meridians\n",
    "meridians = np.arange(180.,360.,0.5)\n",
    "m.drawmeridians(meridians,labels=[0,0,0,1],fontsize=10)\n",
    "m.arcgisimage(service='World_Physical_Map', xpixels = 1500, verbose= True)\n",
    "\n",
    "x,y=m(data_SIR['longitude'].data, data_SIR['latitude'].data)\n",
    "#m.plot(x,y, 'bs', markersize=11, markeredgewidth=1, alpha=0.2)\n",
    "m.scatter(x,y,c=array, s=100, marker='s',lw=0, cmap='Greys', alpha=.8)  # cmap='Blues'\n",
    "#x,y=m(ISAlons,ISAlats)\n",
    "#m.plot(x,y,'ro',markersize=2,markeredgewidth=0.0)\n",
    "plt.title('Tb_'+date+' '+str(obs))\n",
    "\n",
    "# plot a red line on colorbar for the MOD for the GRD pixel\n",
    "cbar=plt.colorbar()\n",
    "cbar.ax.hlines(cbar.norm(GRD), 0, 1, color='red') \n",
    "plt.clim(230,250)  #color bar limits\n",
    "\n",
    "\n",
    "plt.savefig(outFile)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#needs updating \n",
    "outFile = Path(outDir, 'testtestTB2.png')\n",
    "\n",
    "ax = fig.add_axes([0.1,0.1,0.8,0.8])\n",
    "\n",
    "# create polar stereographic Basemap instance.\n",
    "m = Basemap(projection='stere',lon_0=lon_start,lat_0=lat_start,lat_ts=90.,\\\n",
    "            llcrnrlat=(lat_start-.7),urcrnrlat=(lat_start+.7),\\\n",
    "            llcrnrlon=(lon_start-.7),urcrnrlon=(lon_start+.7),\\\n",
    "            rsphere=6371200.,resolution='l',area_thresh=10000, epsg=3857)\n",
    "m.drawcoastlines()\n",
    "m.drawcountries()\n",
    "m.drawstates()\n",
    "#m.etopo(scale=5, alpha=0.7)\n",
    "#m.bluemarble(scale=1)\n",
    "parallels = np.arange(0.,90,0.5)\n",
    "m.drawparallels(parallels,labels=[1,0,0,0],fontsize=10)\n",
    "# draw meridians\n",
    "meridians = np.arange(180.,360.,0.5)\n",
    "m.drawmeridians(meridians,labels=[0,0,0,1],fontsize=10)\n",
    "m.arcgisimage(service='World_Physical_Map', xpixels = 1500, verbose= True)\n",
    "\n",
    "x,y=m(data_SIR['longitude'].data, data_SIR['latitude'].data)\n",
    "#m.plot(x,y, 'bs', markersize=11, markeredgewidth=1, alpha=0.2)\n",
    "m.scatter(x,y,c=MOD_DOY_df['Avg'], s=150, marker='s',lw=0,cmap='Blues', alpha=.8)\n",
    "#x,y=m(ISAlons,ISAlats)\n",
    "#m.plot(x,y,'ro',markersize=2,markeredgewidth=0.0)\n",
    "plt.title('Tb - std dev')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.savefig(outFile)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# PLOT THE EARLY EVENTS MAP\n",
    "# if getting errors, likely because there are 0 or only 1 early melt event for the subset\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "# create figure and axes instances\n",
    "#fig = plt.figure()\n",
    "ax = fig.add_axes([0.1,0.1,0.8,0.8])\n",
    "#ax = fig.add_subplot(121)\n",
    "# create polar stereographic Basemap instance.\n",
    "m = Basemap(projection='stere',lon_0=lon_start,lat_0=lat_start,lat_ts=90.,\\\n",
    "            llcrnrlat=(lat_start-.7),urcrnrlat=(lat_start+.7),\\\n",
    "            llcrnrlon=(lon_start-.7),urcrnrlon=(lon_start+.7),\\\n",
    "            rsphere=6371200.,resolution='l',area_thresh=10000, epsg=3857)\n",
    "m.drawcoastlines()\n",
    "m.drawcountries()\n",
    "m.drawstates()\n",
    "#m.etopo(scale=5, alpha=0.7)\n",
    "#m.bluemarble(scale=1)\n",
    "parallels = np.arange(0.,90,5.)\n",
    "m.drawparallels(parallels,labels=[1,0,0,0],fontsize=10)\n",
    "# draw meridians\n",
    "meridians = np.arange(180.,360.,5.)\n",
    "m.drawmeridians(meridians,labels=[0,0,0,1],fontsize=10)\n",
    "m.arcgisimage(service='World_Physical_Map', xpixels = 1500, verbose= True)\n",
    "\n",
    "x,y=m(data_SIR['longitude'].data, data_SIR['latitude'].data)\n",
    "#m.plot(x,y, 'bs', markersize=11, markeredgewidth=1, alpha=0.2)\n",
    "m.scatter(x,y,c=events_array, s=150, marker='s',lw=0,cmap='RdBu', alpha=.8)\n",
    "#x,y=m(ISAlons,ISAlats)\n",
    "#m.plot(x,y,'ro',markersize=2,markeredgewidth=0.0)\n",
    "plt.title(sensor_SIR+'-'+Site+' - Early Melt Events - (Avg '+str(Years[0])+'-'+str(Years[-1])+')')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_axes([0.1,0.1,0.8,0.8])\n",
    "\n",
    "# create polar stereographic Basemap instance.\n",
    "m = Basemap(projection='stere',lon_0=lon_start,lat_0=lat_start,lat_ts=90.,\\\n",
    "            llcrnrlat=(lat_start-.7),urcrnrlat=(lat_start+.7),\\\n",
    "            llcrnrlon=(lon_start-.7),urcrnrlon=(lon_start+.7),\\\n",
    "            rsphere=6371200.,resolution='l',area_thresh=10000, epsg=3857)\n",
    "m.drawcoastlines()\n",
    "m.drawcountries()\n",
    "m.drawstates()\n",
    "#m.etopo(scale=5, alpha=0.7)\n",
    "#m.bluemarble(scale=1)\n",
    "parallels = np.arange(0.,90,0.5)\n",
    "m.drawparallels(parallels,labels=[1,0,0,0],fontsize=10)\n",
    "# draw meridians\n",
    "meridians = np.arange(180.,360.,0.5)\n",
    "m.drawmeridians(meridians,labels=[0,0,0,1],fontsize=10)\n",
    "m.arcgisimage(service='World_Physical_Map', xpixels = 1500, verbose= True)\n",
    "\n",
    "#x,y=m(data_SIR['longitude'].data, data_SIR['latitude'].data)\n",
    "#m.plot(x,y, 'bs', markersize=11, markeredgewidth=1, alpha=0.2)\n",
    "#m.scatter(x,y,c=MOD_DOY_df['Avg'], s=150, marker='s',lw=0,cmap='Blues', alpha=.8)\n",
    "#x,y=m(ISAlons,ISAlats)\n",
    "#m.plot(x,y,'ro',markersize=2,markeredgewidth=0.0)\n",
    "plt.title(sensor_SIR+'-'+Site+' - Early Melt Events - (Avg '+str(Years[0])+'-'+str(Years[-1])+')')\n",
    "#plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the average Jan-Feb DAV for each pixel for each year - SIR\n",
    "Jan_Feb_DAV=Winter_DAV(data_SIR['TB'], data_SIR['cal_date'], data_SIR['cal_year'], subYears, rows_cols_env)\n",
    "Jan_Feb_DAV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Jan_Feb_DAV_avg=Jan_Feb_DAV.mean(axis=0)\n",
    "Jan_Feb_DAV_avg=Jan_Feb_DAV_avg.values\n",
    "#Jan_Feb_DAV_avg=np.round(Jan_Feb_DAV_avg)\n",
    "Jan_Feb_DAV_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the average Jan-Feb DAV for the GRD pixel for each year\n",
    "Jan_Feb_DAV_GRD=Winter_DAV(data_GRD['TB'], data_GRD['cal_date'], data_GRD['cal_year'], subYears, rows_cols_GRD)\n",
    "Jan_Feb_DAV_GRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg Jan-Feb DAV for all years - GRD pixel\n",
    "Jan_Feb_DAV_GRD_all=np.nanmean(Jan_Feb_DAV_GRD)\n",
    "Jan_Feb_DAV_GRD_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOT WINTER DAV MAP\n",
    "#may need updating but IS almost WORKING 2-23-23, needs a scale bar\n",
    "# if getting errors, likely because there are 0 or only 1 early melt event for the subset\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "# create figure and axes instances\n",
    "#fig = plt.figure()\n",
    "ax = fig.add_axes([0.1,0.1,0.8,0.8])\n",
    "#ax = fig.add_subplot(121)\n",
    "# create polar stereographic Basemap instance.\n",
    "m = Basemap(projection='stere',lon_0=lon_start,lat_0=lat_start,lat_ts=90.,\\\n",
    "            llcrnrlat=(lat_start-.7),urcrnrlat=(lat_start+.7),\\\n",
    "            llcrnrlon=(lon_start-.7),urcrnrlon=(lon_start+.7),\\\n",
    "            rsphere=6371200.,resolution='l',area_thresh=10000, epsg=3857)\n",
    "m.drawcoastlines()\n",
    "m.drawcountries()\n",
    "m.drawstates()\n",
    "#m.etopo(scale=5, alpha=0.7)\n",
    "#m.bluemarble(scale=1)\n",
    "parallels = np.arange(0.,90,0.5)\n",
    "m.drawparallels(parallels,labels=[1,0,0,0],fontsize=10)\n",
    "# draw meridians\n",
    "meridians = np.arange(180.,360.,0.5)\n",
    "m.drawmeridians(meridians,labels=[0,0,0,1],fontsize=10)\n",
    "m.arcgisimage(service='World_Physical_Map', xpixels = 1500, verbose= True)\n",
    "\n",
    "x,y=m(data_SIR['longitude'],data_SIR['latitude'])\n",
    "#m.plot(x,y, 'bs', markersize=11, markeredgewidth=1, alpha=0.2)\n",
    "m.scatter(x,y,c=Jan_Feb_DAV_avg, s=100, marker='s',lw=0,cmap='Blues', alpha=.8)\n",
    "#x,y=m(ISAlons,ISAlats)\n",
    "#m.plot(x,y,'ro',markersize=2,markeredgewidth=0.0)\n",
    "plt.title(sensor_SIR+'-'+Site+' - DAV - (Avg '+str(subYears[0])+'-'+str(subYears[-1])+')')\n",
    "\n",
    "# plot a red line on colorbar for the GRD mean - all years\n",
    "cbar=plt.colorbar()\n",
    "cbar.ax.hlines(cbar.norm(Jan_Feb_DAV_GRD_all), 0, 1, color='green') \n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_axes([0.1,0.1,0.8,0.8])\n",
    "\n",
    "# create polar stereographic Basemap instance.\n",
    "m = Basemap(projection='stere',lon_0=lon_start,lat_0=lat_start,lat_ts=90.,\\\n",
    "            llcrnrlat=(lat_start-.7),urcrnrlat=(lat_start+.7),\\\n",
    "            llcrnrlon=(lon_start-.7),urcrnrlon=(lon_start+.7),\\\n",
    "            rsphere=6371200.,resolution='l',area_thresh=10000, epsg=3857)\n",
    "m.drawcoastlines()\n",
    "m.drawcountries()\n",
    "m.drawstates()\n",
    "#m.etopo(scale=5, alpha=0.7)\n",
    "#m.bluemarble(scale=1)\n",
    "parallels = np.arange(0.,90,0.5)\n",
    "m.drawparallels(parallels,labels=[1,0,0,0],fontsize=10)\n",
    "# draw meridians\n",
    "meridians = np.arange(180.,360.,0.5)\n",
    "m.drawmeridians(meridians,labels=[0,0,0,1],fontsize=10)\n",
    "m.arcgisimage(service='World_Physical_Map', xpixels = 1500, verbose= True)\n",
    "\n",
    "x,y=m(data_SIR['longitude'],data_SIR['latitude'])\n",
    "#m.plot(x,y, 'bs', markersize=11, markeredgewidth=1, alpha=0.2)\n",
    "m.scatter(x,y,c=MOD_DOY_df['Avg'], s=150, marker='s',lw=0,cmap='Blues', alpha=.8)\n",
    "#x,y=m(ISAlons,ISAlats)\n",
    "#m.plot(x,y,'ro',markersize=2,markeredgewidth=0.0)\n",
    "plt.title(sensor_SIR+'-'+Site+' - DAV - (Avg '+str(subYears[0])+'-'+str(subYears[-1])+')')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# winter DAV histogram\n",
    "plt.close()\n",
    "fig,ax=plt.subplots()\n",
    "ax.hist(Jan_Feb_DAV_avg)\n",
    "avg=np.mean(Jan_Feb_DAV_avg)  # calculate average winter DAV all pixels\n",
    "ax.axvline(x=avg, color='blue', linestyle='dashed')  # plot a line for the average winter DAV for all pixels\n",
    "ax.axvline(x=Jan_Feb_DAV_GRD_all, color='red')  # plot a line for winter DAV for GRD pixel\n",
    "ax.set_xlabel('DAV (K)')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Winter (Jan-Feb) DAV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to run this cell for the max DAV to work\n",
    "y_dims_list=list(range(len(data_SIR['TB'][0,:,0])))  # creates a list of the y-dimension pixel indices, used for plotting\n",
    "x_dims_list=list(range(len(data_SIR['TB'][0,0,:])))\t# creates a list of the x-dimension pixel indices, for plotting\n",
    "y_s=list(range(rows_cols_env[0],rows_cols_env[1]))  # makes a list of the y(row) numbers so the dataframe of early melt events can be indexed\n",
    "x_s=list(range(rows_cols_env[2],rows_cols_env[3]))  # makes a list of the x(col) numbers so the dataframe of early melt events can be indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create array of the max DAV for each pixel for all years\n",
    "DAV_frame=pd.DataFrame()\n",
    "for i in y_dims_list:\n",
    "    for j in x_dims_list:\n",
    "        column=pd.DataFrame(data=DAV_SIR[:,i,j], columns=[str(y_s[i])+','+str(x_s[j])])\n",
    "        DAV_frame=pd.concat([DAV_frame,column],axis=1)\n",
    "\n",
    "DAV_frame.rename_axis(columns=\"Row,Col\", inplace=True)\n",
    "DAV_frame['date'] = data_SIR['cal_date']\n",
    "DAV_frame.set_index('date', inplace=True)\n",
    "#DAV_frame=DAV_frame.groupby(pd.Grouper(freq='A')).max() #note to joan might also do sum for the melt flags\n",
    "DAV_max_array=DAV_frame.max().values\n",
    "DAV_max_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAV_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create array of the max DAV for each pixel for year of interest\n",
    "year=2011  #specify year\n",
    "DAV_max_array_year=DAV_frame.loc[str(year)].max().values\n",
    "DAV_max_array_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAV_max_GRD=np.nanmax(DAV_GRD)\n",
    "DAV_max_GRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAV_GRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note this year needs to be the same as the max dav plot below\n",
    "#year=2005\n",
    "DAV_max_GRD_year=np.nanmax(DAV_GRD[data_GRD['cal_year']==year])\n",
    "DAV_max_GRD_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT MAX DAV MAP\n",
    "#probalby neds updating in 2-23-23\n",
    "#set this one up to generate a plot\n",
    "#plot_type='all'\n",
    "year = 2011\n",
    "plot_type=year\n",
    "#NOTE YEAR SPECIFIED UP A FEW LEVELS (~3)\n",
    "#year=2005\n",
    "\n",
    "if plot_type=='all':\n",
    "    array=DAV_max_array\n",
    "    title=sensor_SIR+'-'+Site+' - Max DAV (K) - (Avg of '+str(subYears[0])+'-'+str(subYears[-1])+')'\n",
    "    GRD=DAV_max_GRD\n",
    "elif plot_type==year:\n",
    "\n",
    "    array=DAV_max_array_year\n",
    "    GRD=DAV_max_GRD_year\n",
    "    title=sensor_SIR+'-'+Site+' - Max DAV (K) - ('+str(year)+')'\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "# create figure and axes instances\n",
    "#fig = plt.figure()\n",
    "ax = fig.add_axes([0.1,0.1,0.8,0.8])\n",
    "#ax = fig.add_subplot(121)\n",
    "# create polar stereographic Basemap instance.\n",
    "m = Basemap(projection='stere',lon_0=lon_start,lat_0=lat_start,lat_ts=90.,\\\n",
    "            llcrnrlat=(lat_start-.7),urcrnrlat=(lat_start+.7),\\\n",
    "            llcrnrlon=(lon_start-.7),urcrnrlon=(lon_start+.7),\\\n",
    "            rsphere=6371200.,resolution='l',area_thresh=10000, epsg=3857)\n",
    "m.drawcoastlines()\n",
    "m.drawcountries()\n",
    "m.drawstates()\n",
    "#m.etopo(scale=5, alpha=0.7)\n",
    "#m.bluemarble(scale=1)\n",
    "parallels = np.arange(0.,90,0.5)\n",
    "m.drawparallels(parallels,labels=[1,0,0,0],fontsize=10)\n",
    "# draw meridians\n",
    "meridians = np.arange(180.,360.,0.5)\n",
    "m.drawmeridians(meridians,labels=[0,0,0,1],fontsize=10)\n",
    "m.arcgisimage(service='World_Physical_Map', xpixels = 1500, verbose= True)\n",
    "\n",
    "x,y=m(data_SIR['longitude'],data_SIR['latitude'])\n",
    "      # formerly pixel_lons, pixel_lats)\n",
    "#m.plot(x,y, 'bs', markersize=11, markeredgewidth=1, alpha=0.2)\n",
    "m.scatter(x,y,c=array, s=100, marker='s',lw=0,cmap='Blues', alpha=.8)\n",
    "#x,y=m(ISAlons,ISAlats)\n",
    "#m.plot(x,y,'ro',markersize=2,markeredgewidth=0.0)\n",
    "plt.title(title)\n",
    "\n",
    "# plot a red line on colorbar for the GRD max\n",
    "cbar=plt.colorbar()\n",
    "#cbar.ax.hlines(cbar.norm(DAV_max_GRD), 0, 1, color='red') \n",
    "cbar.ax.hlines(cbar.norm(DAV_max_GRD_year), 0, 1, color='red')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_axes([0.1,0.1,0.8,0.8])\n",
    "\n",
    "# create polar stereographic Basemap instance.\n",
    "m = Basemap(projection='stere',lon_0=lon_start,lat_0=lat_start,lat_ts=90.,\\\n",
    "            llcrnrlat=(lat_start-.7),urcrnrlat=(lat_start+.7),\\\n",
    "            llcrnrlon=(lon_start-.7),urcrnrlon=(lon_start+.7),\\\n",
    "            rsphere=6371200.,resolution='l',area_thresh=10000, epsg=3857)\n",
    "m.drawcoastlines()\n",
    "m.drawcountries()\n",
    "m.drawstates()\n",
    "#m.etopo(scale=5, alpha=0.7)\n",
    "#m.bluemarble(scale=1)\n",
    "parallels = np.arange(0.,90,0.5)\n",
    "m.drawparallels(parallels,labels=[1,0,0,0],fontsize=10)\n",
    "# draw meridians\n",
    "meridians = np.arange(180.,360.,0.5)\n",
    "m.drawmeridians(meridians,labels=[0,0,0,1],fontsize=10)\n",
    "m.arcgisimage(service='World_Physical_Map', xpixels = 1500, verbose= True)\n",
    "\n",
    "x,y=m(data_SIR['longitude'],data_SIR['latitude'])\n",
    "#m.plot(x,y, 'bs', markersize=11, markeredgewidth=1, alpha=0.2)\n",
    "m.scatter(x,y,c=MOD_DOY_df['Avg'], s=150, marker='s',lw=0,cmap='Blues', alpha=.8)\n",
    "#x,y=m(ISAlons,ISAlats)\n",
    "#m.plot(x,y,'ro',markersize=2,markeredgewidth=0.0)\n",
    "plt.title(title)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year=2011\n",
    "outFile = Path(outDir, '%shist%s%s.png' % (sat_SIR, str(year), SiteLabel))\n",
    "\n",
    "# generate histogram - SIR - pick year \n",
    "\n",
    "Tb_threshold=252  #specify threshold be careful it was set above by sensor\n",
    "data = data_SIR['TB'][data_SIR['cal_year']==year] # CETB_data for all pixels in subset\n",
    "data = data[data>=0]\n",
    "bins = range(150,300)  # bins for histogram\n",
    "fig,ax=plt.subplots()\n",
    "ax.axvline(x=Tb_threshold, color='red')\n",
    "ax.hist(data, bins)\n",
    "ax.set_title(str(year)+ ' '+Site+' SIR Histogram')\n",
    "#ax.set_title(prefix+'row'+str(x)+'col'+str(y))\n",
    "ax.set_xlabel('Brightness Temp (K)')\n",
    "\n",
    "plt.savefig(outFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outFile = Path(outDir, '%s_histsirall%s.png' % (SiteLabel, sat_SIR))\n",
    "\n",
    "# generate histogram - SIR - all data\n",
    "\n",
    "#Tb_threshold=252  #set above by sensor\n",
    "data = data_SIR['TB'][:] # CETB_data for all pixels in subset\n",
    "data = data[data>=0]\n",
    "bins = range(150,300)  # bins for histogram\n",
    "fig,ax=plt.subplots()\n",
    "ax.hist(data, bins)\n",
    "ax.set_title('All data SIR Histogram'+' '+Site)\n",
    "ax.axvline(x=Tb_threshold, color='red')\n",
    "#ax.set_title(prefix+'row'+str(x)+'col'+str(y))\n",
    "ax.set_xlabel('Brightness Temp (K)')\n",
    "\n",
    "plt.savefig(outFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outFile = Path(outDir, '%s_HistGRD%s-%s.png' % (SiteLabel, str(subYears[0]), str(subYears[-1])))\n",
    "               \n",
    "# generate histogram - GRD - all data\n",
    "data = data_GRD['TB'][:] # CETB_data for all pixels in subset\n",
    "data = data[data>=0]\n",
    "bins = range(150,300)  # bins for histogram\n",
    "fig,ax=plt.subplots()\n",
    "ax.hist(data, bins)\n",
    "ax.set_title('All data GRD Histogram')\n",
    "ax.axvline(x=Tb_threshold, color='red')\n",
    "#ax.set_title(prefix+'row'+str(x)+'col'+str(y))\n",
    "ax.set_xlabel('Brightness Temp (K)')\n",
    "\n",
    "plt.savefig(outFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min/max/avg histogram of the 64 SIR pixels, plus the GRD pixel\n",
    "year=2011\n",
    "title=str(lat_start)+'_'+str(lon_start)\n",
    "frame=min_max_series(data_GRD['TB'], data_SIR['TB'], data_SIR['cal_date'], data_SIR['cal_year'], year, title)\n",
    "frame['date'] = data_SIR['cal_date']\n",
    "frame.set_index('date', inplace=True)\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some kind of numpy pandas compatibility issue\n",
    "frame.index\n",
    "frame['min'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outFile = Path(outDir, '%s_SIRhistdis%syear.png' % (SiteLabel, str(subYears[-1] - subYears[0] + 1)))\n",
    "frame.plot.hist(alpha=0.8, bins=100, histtype='step', ylim=[0,100], xlim=[150,320], title=title)\n",
    "plt.savefig(outFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "year=2011\n",
    "outFile = Path(outDir, '%s_tsplot.png' % SiteLabel)\n",
    "\n",
    "# min/max/avg time series  of the 64 SIR pixels, plus the GRD pixel\n",
    "# plot time series of min/max/mean of the 3 km pixels in subset and the 25km (GRD) pixel that envelopes them    \n",
    "frame.plot(xlim=[str(year)+'-01-01',str(year)+'-12-31'], ylim=[150,320], title=title)\n",
    "plt.savefig(outFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TIME SERIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tb and DAV time series for GRD pixel, pick one year\n",
    "year=2010\n",
    "#Tb_threshold=252\n",
    "#DAV_threshold=18\n",
    "TbDAV_series_one_year(data_GRD['TB'],DAV_GRD, data_GRD['cal_date'], data_GRD['cal_year'], year, Tb_threshold, DAV_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tb and DAV time series for SIR pixels, pick one year\n",
    "#year=2003\n",
    "#Tb_threshold=252\n",
    "#DAV_threshold=18\n",
    "#outFile = Path(outDir, 'SevZ_AWStimeseriesALLSIRplots2000.png')\n",
    "TbDAV_series_one_year(data_SIR['TB'], DAV_SIR, data_SIR['cal_date'], data_SIR['cal_year'], year, Tb_threshold, DAV_threshold)\n",
    "#plt.savefig(outFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tb and DAV time series of the single SIR pixel closest to the user-specified lat/lon at top of notebook\n",
    "year=2011\n",
    "#Tb_threshold=252\n",
    "#DAV_threshold=18\n",
    "data=read_Tb_whole(datadir_SIR, prefix_SIR, subYears,\n",
    "                   y_start=rows_cols[0],\n",
    "                   y_end=rows_cols[1],\n",
    "                   x_start=rows_cols[2],\n",
    "                   x_end=rows_cols[3])\n",
    "\n",
    "SIR_1px=data['TB']\n",
    "DAV_1px=calc_DAV(SIR_1px)\n",
    "TbDAV_series_one_year(SIR_1px, DAV_1px, data['cal_date'], data['cal_year'], year, Tb_threshold, DAV_threshold) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TB_SIR_1px=pd.DataFrame(SIR_1px.flatten(), \n",
    "                        columns=[Site_nearest_row_col], \n",
    "                        index=data['cal_date'])\n",
    "TB_SIR_1px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAV_SIR_1px=pd.DataFrame(DAV_1px.flatten(),\n",
    "                         columns=[Site_nearest_row_col],\n",
    "                         index=data['cal_date'])\n",
    "DAV_SIR_1px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cal_date'], data_GRD['cal_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE CSV of GRD and SIR TB, GRD and SIR64 ST, and GRD And SIR DAV\n",
    "\n",
    "# CREATE CSV OF TB, DAV, SD and sirSD FOR THE GRD PIXEL - ALL YEARS\n",
    "Tb_frame = pd.DataFrame(data_GRD['TB'][:,0,0], index=data_GRD['cal_date'])\n",
    "DAV_frame = pd.DataFrame(DAV_GRD[:,0,0], index=data_GRD['cal_date'])\n",
    "GRD_data_out = pd.concat([Tb_frame,DAV_frame,TB_SIR_1px,DAV_SIR_1px,Tb_sd_frame_GRD,Tb_std_64,Tb_sd_diff],\n",
    "                         axis=1)\n",
    "GRD_data_out.columns = ['GRDTb36V','GRDDAV36V','SIRTB36V','SIRDAV36V',\n",
    "                        'GRD_SD_36V','SIR64_to_SD_36V','SD_diff[GRD-SIR]']\n",
    "GRD_data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outFile = Path(outNoisy, 'sd', \n",
    "               sat_GRD+'_'+sensor_GRD+'-'+channel_GRD+'-'+alg_GRD+'_'\n",
    "               +str(lat_start)+'_'+str(lon_start)+'DATA.csv')  # modify destination to save csv\n",
    "GRD_data_out.to_csv(outFile)\n",
    "outFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CREATE CSV OF TB FOR THE GRD PIXEL - ALL YEARS\n",
    "Tb_frame=pd.DataFrame(data_GRD['TB'][:,0,0], index=data_GRD['cal_date'])\n",
    "outFile = Path(outNoisy, 'noisytb_csv',\n",
    "               sat_GRD+'_'+sensor_GRD+'-'+channel_GRD+'-'+alg_GRD+'_'\n",
    "               +str(lat_start)+'_'+str(lon_start)+'.csv')  # modify destination to save csv\n",
    "Tb_frame.to_csv(outFile)\n",
    "outFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CREATE CSV of DAV FOR THE GRD PIXEL - ALL YEARS\n",
    "DAV_frame=pd.DataFrame(DAV_GRD[:,0,0], index=data['cal_date'])\n",
    "outFile = Path(outNoisy, 'noisydav_csv',\n",
    "               sat_GRD+'_'+sensor_GRD+'-'+channel_GRD+'-'+alg_GRD+'_'\n",
    "               +str(lat_start)+'_'+str(lon_start)+'.csv') # modify destination to save csv\n",
    "DAV_frame.to_csv(outFile)\n",
    "outFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE CSV of TB FOR 64pixel subset OF SIR PIXELS - ALL YEARS\n",
    "Tb_frame=pd.DataFrame()\n",
    "for i in y_dims_list:\n",
    "    for j in x_dims_list:\n",
    "        column=pd.DataFrame(data=data_SIR['TB'][:,i,j], columns=[str(y_s[i])+','+str(x_s[j])])\n",
    "        Tb_frame=pd.concat([Tb_frame,column],axis=1)\n",
    "Tb_frame=Tb_frame.set_index(data_SIR['cal_date'])\n",
    "outFile = Path(outNoisy, 'noisytbSIR_csv',\n",
    "               sat_GRD+'_'+sensor_GRD+'-'+channel_GRD+'-'+alg_GRD+'_'\n",
    "               +str(lat_start)+'_'+str(lon_start)+'.csv')  # modify destination to save csv\n",
    "Tb_frame.to_csv(outFile)\n",
    "outFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE CSV of DAV FOR 64pixel subset OF SIR PIXELS - ALL YEARS\n",
    "DAV_frame=pd.DataFrame()\n",
    "for i in y_dims_list:\n",
    "    for j in x_dims_list:\n",
    "        column=pd.DataFrame(data=DAV_SIR[:,i,j], columns=[str(y_s[i])+','+str(x_s[j])])\n",
    "        DAV_frame=pd.concat([DAV_frame,column],axis=1)\n",
    "DAV_frame=DAV_frame.set_index(data_SIR['cal_date'])\n",
    "outFile = Path(outNoisy, 'noisydavSIR_csv',\n",
    "               sat_GRD+'_'+sensor_GRD+'-'+channel_GRD+'-'+alg_GRD+'_'\n",
    "               +str(lat_start)+'_'+str(lon_start)+'.csv')  # modify destination to save csv\n",
    "DAV_frame.to_csv(outFile)\n",
    "outFile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
