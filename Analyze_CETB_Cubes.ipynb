{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze_CETB_Cubes\n",
    "\n",
    "Calculate SIR and/or GRD analysis by year for selected subset areas. For example, running melt-onset-dates by year and pixel, or calculating intrapixel stddevs.\n",
    "\n",
    "Saves analysis data with geolocation information in pickle files.\n",
    "\n",
    "Makes geotiff maps of various annual and/or average results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in all the modules needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab notebook\n",
    "# check if a windows machine, it needs special attention\n",
    "# this extra step will bypass an error from mpl_toolkits.basemap\n",
    "import os\n",
    "if os.name == 'nt':\n",
    "    os.environ[\"PROJ_LIB\"] = os.path.join(os.environ[\"CONDA_PREFIX\"], \"Library\", \"share\")\n",
    "    os.environ[\"GDAL_DATA\"] = os.path.join(os.environ[\"CONDA_PREFIX\"], \"Library\", \"share\", \"gdal\")\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from netCDF4 import Dataset, num2date\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "from cetbtools.ease2conv import Ease2Transform\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set parameters to display more than default rows/cols in Data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 999)\n",
    "pd.set_option('display.min_rows', 200)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 120)\n",
    "pd.set_option('display.max_colwidth', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the local machine location of CETB data cubes\n",
    "# This directory is expected to contain subdirectories in the following hierarchy\n",
    "# that duplicates the hierarchy on the Google Shared Drive NSIDC-SD-CETB/v1/, \n",
    "# for example:\n",
    "# dataDir/F13_SSMI/N/nc_cubes/cubes_<regionName>\n",
    "user = 'Joan' #Mariah #MJWindows\n",
    "if ('Joan' == user):\n",
    "    dataDir = '/mnt/data3/cetb/nsidc0630_v1/' #jmr machine fringe \n",
    "    scriptDir = Path(Path.home(), 'ipynb_melt_onset', 'scripts')\n",
    "    outDir = Path(Path.home(), 'MODs')\n",
    "elif ('Mariah' == user):\n",
    "    dataDir = Path(Path.home(), 'nsidc0630_v1') # Mariah's PC or Mary Jo's Mac\n",
    "    scriptDir = Path(Path.home(), 'ipynb_melt_onset', 'scripts')\n",
    "    outDir = Path(Path.home(), 'nsidc0630_v1', 'MODs')\n",
    "elif ('MJWindows' == user):\n",
    "    dataDir = Path('Z:/mj On My Mac/nsidc0630_v1') # Mary Jo's Windows machine\n",
    "    scriptDir = Path(Path.home(), 'ipynb_melt_onset', 'scripts')\n",
    "    outDir = ''\n",
    "else:\n",
    "    raise ValueError(\"unknown user= %s\\n\" % (user) )\n",
    "    \n",
    "%cd $scriptDir\n",
    "dataDir, outDir, user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the custom functions\n",
    "from CETB_IO import read_Tb_whole\n",
    "from CETB_IO import coords\n",
    "from CETB_algorithms import calc_DAV\n",
    "# from CETB_IO import find_cube_offset\n",
    "from CETB_IO import grid_locations_of_subset\n",
    "from CETB_IO import years_for\n",
    "from CETB_IO import write_MOD_df_to_geotiff\n",
    "from CETB_algorithms import DAV_MOD\n",
    "from CETB_analysis import MOD_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify inputs\n",
    "\n",
    "This cell is the place to specify the cube name to analyze ('WesternCA', 'AKYukon', etc)\n",
    "and the sensor and channels to process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify region, satellite, sensor, channel, and image reconstruction algorithm of interest in file name\n",
    "# this notebook will read in 2 CETB datasets so that channels/algorithms/sensors can be compared\n",
    "region='AKYukon'#'Laptev' #'WesternCA'  #'GLaIL'  #make this the same syntax as cubefilenames and sub-directory\n",
    "sat_GRD='F18'   #'AQUA' for AMSRE, 'F13','F14','F15'... for SSMI\n",
    "sat_SIR= 'F18'\n",
    "sensor_GRD='SSMIS'  #'AMSRE', 'SSMI', etc.\n",
    "sensor_SIR='SSMIS'\n",
    "channel_GRD='37V'  #'36V','36H', '18V','18H', etc. '19V','19H' and '37V','37H' for SSMI)\n",
    "channel_SIR='37V'\n",
    "alg_GRD='GRD'   #SIR or GRD\n",
    "alg_SIR='SIR'\n",
    "\n",
    "# set the sir to grd factor, depends on the channel\n",
    "if (re.match('^[389]', channel_GRD)):\n",
    "    sir_2_grd_factor = 8 # assume 3.125 km to 25 km\n",
    "elif (re.match('^[12]', channel_GRD)):\n",
    "    sir_2_grd_factor = 4 # assume 6.25 km to 25 km\n",
    "else:\n",
    "    raise ValueError(\"Cannot determine sir_2_grd_factor from channel %s\\n\" % (channel_GRD) )\n",
    "\n",
    "cubeType_GRD = channel_GRD + '-' + alg_GRD\n",
    "cubeType_SIR = channel_SIR + '-' + alg_SIR\n",
    "  \n",
    "if ('SSMI' == sensor_GRD) or ('SSMIS' == sensor_GRD):\n",
    "    provider='CSU' \n",
    "    version='v1.*'\n",
    "elif 'AMSRE' == sensor_GRD:\n",
    "    provider='RSS'\n",
    "    version='v1.3'\n",
    "\n",
    "hemName = 'N'    \n",
    "\n",
    "# on Joan's machine\n",
    "#datadir_GRD = dataDir + sat_GRD+'_'+sensor_GRD+'/'+region+'/' \n",
    "#datadir_SIR = dataDir + sat_SIR+'_'+sensor_SIR+'/'+region+'/' \n",
    "# on MJ's machine\n",
    "datadir_GRD = \"%s/%s_%s/%s/nc_cubes/cubes_%s/\" % (\n",
    "    dataDir, sat_GRD, sensor_GRD, hemName, region )\n",
    "datadir_SIR = \"%s/%s_%s/%s/nc_cubes/cubes_%s/\" % (\n",
    "    dataDir, sat_SIR, sensor_SIR, hemName, region )\n",
    "\n",
    "# prefix filepath\n",
    "prefix_GRD = 'CETB.cubefile.'+region+'.'+sat_GRD+'_'+sensor_GRD+'-'+channel_GRD+'-'+alg_GRD+'-'+provider+'-'+version\n",
    "prefix_SIR = 'CETB.cubefile.'+region+'.'+sat_SIR+'_'+sensor_SIR+'-'+channel_SIR+'-'+alg_SIR+'-'+provider+'-'+version\n",
    "\n",
    "Years=years_for(sat_GRD)\n",
    "#might want to truncate Years to subset if very slow during testing\n",
    "#if we give it more years than available what do we want it to do? \n",
    "#warn me but return what it finds\n",
    "\n",
    "# Truncate Years here for speed \n",
    "subYears = Years[0:] \n",
    "#subYears = Years\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subYears"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the geographic bounds of the subset area inside the cube to process\n",
    "\n",
    "Also set the 'Site' name for identifying output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPECIFY latitude and longitude in decimal degrees, need to choose lat/lon corners so that we will load\n",
    "# in a rectangle of pixels within the corners of these coordinates\n",
    "# These areaname values will be used in output filenames, so they should be relatively short, and should\n",
    "# not include spaces or other punctuation\n",
    "areaname='barrow' \n",
    "\n",
    "if ('vatna' == areaname):\n",
    "    lat_start=63.75  \n",
    "    lat_end=64.88    \n",
    "    lon_start=-20 \n",
    "    lon_end=-15  \n",
    "    #Enter a site name for titles of plots\n",
    "    Site='Vatnajokull, Iceland'\n",
    "elif 'hunza' == areaname:\n",
    "    lat_start=35.9  \n",
    "    lat_end=37.1   \n",
    "    lon_start=74 \n",
    "    lon_end=76 \n",
    "    #Enter a site name for titles of plots\n",
    "    Site='Hunza Basin'\n",
    "elif 'gsl' == areaname:\n",
    "    lat_start=59.00  \n",
    "    lat_end=67.00   \n",
    "    lon_start=-119.00 \n",
    "    lon_end=-107.00\n",
    "    #Enter a site name for titles of plots\n",
    "    Site='Great Slave Lake, Canada'\n",
    "elif 'bathurst_range' == areaname:\n",
    "    lat_start=58.00  \n",
    "    lat_end=69.00   \n",
    "    lon_start=-125.00 \n",
    "    lon_end=-106.00\n",
    "    #Enter a site name for titles of plots\n",
    "    Site='Bathurst Caribou Range, NWT'\n",
    "elif 'sz' == areaname:\n",
    "    lat_start=77.00  \n",
    "    lat_end=81.00   \n",
    "    lon_start=89.00 \n",
    "    lon_end=108.00\n",
    "    #Enter a site name for titles of plots\n",
    "    Site='Severnaya Zemlya, Russia'\n",
    "elif 'barrow' == areaname:\n",
    "    lat_start=69.50  \n",
    "    lat_end=71.50    \n",
    "    lon_start=-158 \n",
    "    lon_end=-152  \n",
    "    #Enter a site name for titles of plots\n",
    "    Site='Barrow/Utkiagvik, AK'\n",
    "elif ('kuparuk' == areaname):\n",
    "    lat_start=68.50  \n",
    "    lat_end=70.50    \n",
    "    lon_start=-151 \n",
    "    lon_end=-148  \n",
    "    #Enter a site name for titles of plots\n",
    "    Site='Kuparuk Basin, AK'\n",
    "else: \n",
    "    raise ValueError(\"Unknown area name=%s\" % (areaname) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MJB: I think this cell is obsolete, since the output is never used \n",
    "# get cube offset for finding row/col\n",
    "# function is region specific\n",
    "# find_cube_offset(region, cubeDir=datadir_SIR, cubeType=cubeType_SIR, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir_GRD, prefix_GRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the GRD pixel IDs for the lat/lon rectangle chosen\n",
    "# and then calculate the corrsponding SIR pixel row/col numbers\n",
    "rows_cols_GRD=coords(datadir_GRD, prefix_GRD, lat_start, lat_end, lon_start, lon_end)\n",
    "rows_cols_env = tuple(np.array(rows_cols_GRD) * sir_2_grd_factor)\n",
    "print(rows_cols_GRD)\n",
    "print(rows_cols_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load GRD Tb data\n",
    "data_GRD = read_Tb_whole(datadir_GRD, prefix_GRD, subYears,\n",
    "                         rows_cols_GRD[0], rows_cols_GRD[1], rows_cols_GRD[2], rows_cols_GRD[3])\n",
    "\n",
    "# load in SIR TB data\n",
    "data_SIR = read_Tb_whole(datadir_SIR, prefix_SIR, subYears,\n",
    "                         rows_cols_env[0], rows_cols_env[1], rows_cols_env[2], rows_cols_env[3])\n",
    "\n",
    "# Information passed back from \"read_Tb_whole\" reader includes:\n",
    "# CETB_SIR = data_SIR['TB']   # 3-D Tb time-series array of TB\n",
    "# data_SIR['cal_date']    # 1-D array of dates, these will get passed to later functions\n",
    "# data_SIR['cal_year']    # 1-D array of years\n",
    "# data_SIR['cal_month']   # 1-D array of months\n",
    "# data_SIR['latitude'], data_SIR['longitude'] # 2-D arrays of subset pixel lat/lons\n",
    "# data_SIR['x'], data_SIR['y'] # 2-D arrays of subset pixel projected x/y\n",
    "# data_SIR['gpd'] # name of EASE2 projection that the subset was derived from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate DAV for the Tb data that was imported\n",
    "DAV_GRD = calc_DAV(data_GRD['TB'])\n",
    "DAV_SIR = calc_DAV(data_SIR['TB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAV_GRD.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Placeholder here to calculate the std dev of the 64 SIR pixels in each GRD pixel\n",
    "## Will also need to decide how to save this additional information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the MOD parameters\n",
    "\n",
    "window : window for MOD algorithm, default is 1, '10' would be 5 days (remember that the CETB data come in 2 measurements per day)\n",
    "\n",
    "count : number of Tb/DAV exceedances needed to trigger melt-onset-date\n",
    "\n",
    "DAV and TB thresholds here are from publications:\n",
    "\n",
    "Johnson et al 2020 AMSRE rSIR Tb >= 249 DAV>=13 and AMSRE GRD Tb>=243 DAV>=14\n",
    "\n",
    "From Johnson et al 2020 SSMI rSIR and GRD Tb>=247 DAV>=10\n",
    "\n",
    "DAV_threshold : diurnal amplitude variation in Kelvins default is 10 (per pubs)\n",
    "Tb_threshold : TB threshold above which melt is possibly triggered\n",
    "\n",
    "Colorado (Johnson et al 2020) used 5 times in 7 day window\n",
    "Patagonia (Monahan and Ramage 2010) and Yukon (Semmens et al 2013?) used 3 times in 5 day window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 1   \n",
    "count = 1  \n",
    "# These thresholds are good to use for SSM/I\n",
    "# TODO: this could also be an if block that sets a default for AMSR-E/AMSR2 as well\n",
    "DAV_threshold = 10\n",
    "Tb_threshold = 247"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data frames of MOD (or potentially other variables/analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MOD of the GRD pixel - avg all years\n",
    "MOD_DOY_array_GRD, MOD_DOY_GRD_df, meltflag_GRD_df = MOD_array(\n",
    "    datadir_GRD, prefix_GRD, data_GRD, DAV_GRD, rows_cols_GRD, \n",
    "    subYears, window, count, DAV_threshold, Tb_threshold)\n",
    "MOD_DOY_GRD_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the meltflag dataframe \n",
    "\n",
    "Has an entry for each date (morning and evening) on rows, and for each pixel on columns \n",
    "\n",
    "It contains a 1 for any location/date that the melt criteria were met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meltflag_GRD_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sir MOD array - MOD will be in day of year (DOY) #changeback to Years for all years\n",
    "MOD_DOY_array, MOD_DOY_df, meltflag_df = MOD_array(\n",
    "    datadir_SIR, prefix_SIR, data_SIR, DAV_SIR, rows_cols_env, \n",
    "    subYears, window, count, DAV_threshold, Tb_threshold)\n",
    "MOD_DOY_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes about changes from original notebooks:\n",
    "\n",
    "The old notebooks used to call MOD_array to get the average MOD for a set of years \n",
    "and then call MOD_array_year for a given year of interest.\n",
    "\n",
    "Now we just call MOD_array for SIR data and GRD data, and get back a data frame with \n",
    "MOD columns for each individual year, and one column for the avg MOD for all the years.\n",
    "\n",
    "This will run much faster, and can be saved and just re-read from a saved file on disk.\n",
    "\n",
    "Now we are setting up to save these data and the way to read them in another notebook is:\n",
    "\n",
    "new = pd.read_pickle(MOD_DOY_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the MOD by year data frames for SIR and GRD to pickle files\n",
    "\n",
    "Also saving geolocation and melt onset flag data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "\n",
    "MODinfo = \"MOD_C%1dW%02dT%03dD%02d\" % (count, window, Tb_threshold, DAV_threshold)  \n",
    "meltflaginfo = \"meltflag_C%1dW%02dT%03dD%02d\" % (count, window, Tb_threshold, DAV_threshold) \n",
    "\n",
    "sirMODBasename = \"%s/%s.%s.%s.%s.%s.%s\" % (\n",
    "    outDir, data_SIR['gpd'], region, areaname, sat_SIR, channel_SIR, MODinfo)\n",
    "grdMODBasename = \"%s/%s.%s.%s.%s.%s.%s\" % (\n",
    "    outDir, data_GRD['gpd'], region, areaname, sat_GRD, channel_GRD, MODinfo)\n",
    "\n",
    "sirmeltflagBasename = \"%s/%s.%s.%s.%s.%s.%s\" % (\n",
    "    outDir, data_SIR['gpd'], region, areaname, sat_SIR, channel_SIR, meltflaginfo)\n",
    "grdmeltflagBasename = \"%s/%s.%s.%s.%s.%s.%s\" % (\n",
    "    outDir, data_GRD['gpd'], region, areaname, sat_GRD, channel_GRD, meltflaginfo)\n",
    "\n",
    "filename = \"%s.pkl\" % (sirMODBasename)\n",
    "MOD_DOY_df.to_pickle(filename)\n",
    "print(\"MOD_DOY dataframe saved to %s\\n\" % filename)\n",
    "\n",
    "filename = \"%s.pkl\" % (grdMODBasename)\n",
    "MOD_DOY_GRD_df.to_pickle(filename)\n",
    "print(\"MOD_DOY_GRD dataframe saved to %s\\n\" % filename)\n",
    "\n",
    "filename = \"%s.pkl\" % (sirmeltflagBasename)\n",
    "meltflag_df.to_pickle(filename)\n",
    "print(\"meltflag_df dataframe saved to %s\\n\" % filename)\n",
    "\n",
    "filename = \"%s.pkl\" % (grdmeltflagBasename)\n",
    "meltflag_GRD_df.to_pickle(filename)\n",
    "print(\"meltflag_GRD_df dataframe saved to %s\\n\" % filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optionally, save the MOD for each year as a geotiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outSIR = write_MOD_df_to_geotiff(MOD_DOY_df, data_SIR['gpd'], sirMODBasename, verbose=True)\n",
    "outGRD = write_MOD_df_to_geotiff(MOD_DOY_GRD_df, data_GRD['gpd'], grdMODBasename, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Other things to potentially save here\n",
    "\n",
    "#Examples include: saving the DAV, or saving the std dev\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
