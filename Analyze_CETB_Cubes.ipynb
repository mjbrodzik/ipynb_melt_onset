{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze_CETB_Cubes\n",
    "\n",
    "Calculate SIR and/or GRD analysis by year for selected subset areas. For example, running melt-onset-dates by year and pixel, or calculating intrapixel stddevs.\n",
    "\n",
    "Use this notebook for any analysis/display that is examining the TBs from the cubefiles.\n",
    "\n",
    "Saves MOD/EHD DAV booleans with geolocation information in pickle files that can be examined/displayed elsewhere.\n",
    "\n",
    "Makes geotiff maps of various annual and/or average results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in all the modules needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# check if a windows machine, it needs special attention\n",
    "# this extra step will bypass an error from mpl_toolkits.basemap\n",
    "import os\n",
    "if os.name == 'nt':\n",
    "    os.environ[\"PROJ_LIB\"] = os.path.join(os.environ[\"CONDA_PREFIX\"], \"Library\", \"share\")\n",
    "    os.environ[\"GDAL_DATA\"] = os.path.join(os.environ[\"CONDA_PREFIX\"], \"Library\", \"share\", \"gdal\")\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from netCDF4 import Dataset, num2date\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "from cetbtools.ease2conv import Ease2Transform\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set parameters to display more than default rows/cols in Data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 999)\n",
    "pd.set_option('display.min_rows', 200)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 120)\n",
    "pd.set_option('display.max_colwidth', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the local machine location of CETB data cubes\n",
    "# This directory is expected to contain subdirectories in the following hierarchy\n",
    "# that duplicates the hierarchy on the Google Shared Drive NSIDC-SD-CETB/v1/, \n",
    "# for example:\n",
    "# dataDir/F13_SSMI/N/nc_cubes/cubes_<regionName>\n",
    "user = 'Joan' #Mariah #MJWindows #MAHMac\n",
    "if ('Joan' == user):\n",
    "    dataDir = Path(Path.home(), 'ceph', 'jmr204group','CETB_cubes')\n",
    "    #dataDir = '/mnt/data3/cetb/nsidc0630_v1/' #jmr machine fringe \n",
    "    scriptDir = Path(Path.home(), 'ipynb_melt_onset', 'scripts')\n",
    "    outDir = Path(Path.home(), 'cetb/ipynb_melt__onset_plots')\n",
    "elif ('Mariah' == user):\n",
    "    dataDir = Path(Path.home(), 'nsidc0630_v1') # Mariah's PC or Mary Jo's Mac\n",
    "    scriptDir = Path(Path.home(), 'ipynb_melt_onset', 'scripts')\n",
    "    outDir = Path(Path.home(), 'nsidc0630_v1', 'MODs')\n",
    "elif ('MJWindows' == user):\n",
    "    dataDir = Path('Z:/mj On My Mac/nsidc0630_v1') # Mary Jo's Windows machine\n",
    "    scriptDir = Path(Path.home(), 'ipynb_melt_onset', 'scripts')\n",
    "    outDir = ''\n",
    "elif ('MJMac' == user):\n",
    "    dataDir = Path(Path.home(), 'nsidc0630_v1') # Mary Jo's Mac\n",
    "    scriptDir = Path(Path.home(), 'ipynb_melt_onset', 'scripts')  \n",
    "    outDir = Path(Path.home(), 'nsidc0630_v1', 'pkls')\n",
    "elif ('MAHMac' == user):\n",
    "    dataDir = Path(Path.home(), 'nsidc0630_v1') # Molly's Mac\n",
    "    scriptDir = Path(Path.home(), 'Projects', 'ipynb_melt_onset', 'scripts')  \n",
    "    outDir = Path(Path.home(), 'nsidc0630_v1', 'pkls')\n",
    "else:\n",
    "    raise ValueError(\"unknown user= %s\\n\" % (user) )\n",
    "    \n",
    "%cd $scriptDir\n",
    "dataDir, outDir, user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the custom functions\n",
    "from CETB_IO import read_Tb_whole\n",
    "from CETB_IO import coords\n",
    "from CETB_algorithms import calc_DAV\n",
    "from CETB_IO import grid_locations_of_subset\n",
    "from CETB_IO import years_for\n",
    "from CETB_IO import get_sir_info\n",
    "from CETB_IO import write_df_to_geotiff\n",
    "from CETB_IO import get_site_boundaries\n",
    "from CETB_algorithms import DAV_MOD\n",
    "from CETB_analysis import MOD_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify inputs\n",
    "\n",
    "This cell is the place to specify the cube name to analyze ('WesternCA', 'AKYukon', etc)\n",
    "and the sensor and channels to process.\n",
    "\n",
    "Here is the current map of coverages of our subset cubes for the Northern Hemisphere:\n",
    "\n",
    "<img src='graphics/CETB_EASE2_N_cubes_geolocations.v2.png' width=\"800\" height=\"800\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify region, satellite, sensor, channel, and image reconstruction algorithm of interest in file name\n",
    "# this notebook will read in 2 CETB datasets so that channels/algorithms/sensors can be compared\n",
    "region='EEurope' #'AKYukon' #'Laptev' #'WesternCA'  #'GLaIL'  #make this the same syntax as cubefilenames and sub-directory\n",
    "sat_GRD='GCOMW1'   #'AQUA' for AMSRE, 'F13','F14','F15'... for SSMI 'F18' 'GCOMW1\n",
    "sat_SIR= 'GCOMW1'\n",
    "sensor_GRD='AMSR2'  #'AMSRE', 'SSMI', SSMIS, etc. AMSR2\n",
    "sensor_SIR='AMSR2'\n",
    "channel_GRD='36V'  #'36V','36H', '18V','18H', etc. '19V','19H' and '37V','37H' for SSMI)\n",
    "channel_SIR='36V'\n",
    "alg_GRD='GRD'   #SIR or GRD\n",
    "alg_SIR='SIR'\n",
    "\n",
    "hemName = 'N' \n",
    "\n",
    "# get sir to grd factor and sir_gpd name\n",
    "sir_2_grd_factor, sir_gpd = get_sir_info(channel_SIR, hem=hemName)\n",
    "print(\"channel=%s, sir_2_grd_factor=%d, sir_gpd=%s\" % (\n",
    "    channel_SIR, sir_2_grd_factor, sir_gpd))\n",
    "\n",
    "cubeType_GRD = channel_GRD + '-' + alg_GRD\n",
    "cubeType_SIR = channel_SIR + '-' + alg_SIR\n",
    "  \n",
    "if ('SSMI' == sensor_GRD) or ('SSMIS' == sensor_GRD):\n",
    "    provider='CSU' \n",
    "    version='v1.*'\n",
    "elif 'AMSRE' == sensor_GRD:\n",
    "    provider='RSS'\n",
    "    version='v1.3'\n",
    "elif 'AMSR2' == sensor_GRD:\n",
    "    provider='PPS_XCAL'\n",
    "    version='v1.*'\n",
    "    \n",
    "# on Joan's machine\n",
    "#datadir_GRD = dataDir + sat_GRD+'_'+sensor_GRD+'/'+region+'/' \n",
    "#datadir_SIR = dataDir + sat_SIR+'_'+sensor_SIR+'/'+region+'/' \n",
    "# on MJ's machine\n",
    "datadir_GRD = \"%s/%s_%s/%s/nc_cubes/cubes_%s/\" % (\n",
    "    dataDir, sat_GRD, sensor_GRD, hemName, region )\n",
    "datadir_SIR = \"%s/%s_%s/%s/nc_cubes/cubes_%s/\" % (\n",
    "    dataDir, sat_SIR, sensor_SIR, hemName, region )\n",
    "\n",
    "# prefix filepath\n",
    "prefix_GRD = 'CETB.cubefile.'+region+'.'+sat_GRD+'_'+sensor_GRD+'-'+channel_GRD+'-'+alg_GRD+'-'+provider+'-'+version\n",
    "prefix_SIR = 'CETB.cubefile.'+region+'.'+sat_SIR+'_'+sensor_SIR+'-'+channel_SIR+'-'+alg_SIR+'-'+provider+'-'+version\n",
    "\n",
    "Years=years_for(sat_GRD)\n",
    "#might want to truncate Years to subset if very slow during testing\n",
    "#if we give it more years than available what do we want it to do? \n",
    "#warn me but return what it finds\n",
    "\n",
    "# Only truncate Years here for speed and/or testing\n",
    "subYears = Years[0:2] \n",
    "#subYears = Years\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subYears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_site_boundaries('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the geographic bounds of the subset area inside the cube to process\n",
    "\n",
    "Also set the 'Site' name for identifying output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SiteLabel='NEUkraine'\n",
    "lat_start, lat_end, lon_start, lon_end, Site=get_site_boundaries(SiteLabel)\n",
    "lat_start, lat_end, lon_start, lon_end, Site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir, datadir_GRD, prefix_GRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the GRD pixel IDs for the lat/lon rectangle chosen\n",
    "# and then calculate the corrsponding SIR pixel row/col numbers\n",
    "rows_cols_GRD=coords(datadir_GRD, prefix_GRD, lat_start, lat_end, lon_start, lon_end)\n",
    "rows_cols_env = tuple(np.array(rows_cols_GRD) * sir_2_grd_factor)\n",
    "print(rows_cols_GRD)\n",
    "print(rows_cols_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load GRD Tb data\n",
    "data_GRD = read_Tb_whole(datadir_GRD, prefix_GRD, subYears,\n",
    "                         rows_cols_GRD[0], rows_cols_GRD[1], rows_cols_GRD[2], rows_cols_GRD[3])\n",
    "\n",
    "# load in SIR TB data\n",
    "data_SIR = read_Tb_whole(datadir_SIR, prefix_SIR, subYears,\n",
    "                         rows_cols_env[0], rows_cols_env[1], rows_cols_env[2], rows_cols_env[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate DAV for the Tb data that was imported\n",
    "DAV_GRD = calc_DAV(data_GRD['TB'])\n",
    "DAV_SIR = calc_DAV(data_SIR['TB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine shapes of the dataFrames and the DAV (masked arrays)--these are (timeSteps, rows, cols)\n",
    "data_SIR['TB'].shape, DAV_SIR.shape, data_GRD['TB'].shape, DAV_GRD.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Placeholder here to calculate the std dev of the 64 SIR pixels in each GRD pixel\n",
    "## Will also need to decide how to save this additional information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the MOD and EHD parameters\n",
    "\n",
    "window : window for MOD algorithm, '10' would be 5 days (remember that the CETB data come in 2 measurements per day)\n",
    "\n",
    "count : number of Tb/DAV exceedances needed to trigger melt-onset-date\n",
    "\n",
    "DAV and TB thresholds here are from these publications:\n",
    "\n",
    "Johnson et al 2020 AMSRE rSIR Tb >= 249 DAV>=13 and AMSRE GRD Tb>=243 DAV>=14\n",
    "\n",
    "From Johnson et al 2020 SSMI rSIR and GRD Tb>=247 DAV>=10\n",
    "\n",
    "DAV_threshold : diurnal amplitude variation in Kelvins default is 10 (per pubs)\n",
    "Tb_threshold : TB threshold above which melt is possibly triggered\n",
    "\n",
    "Colorado (Johnson et al 2020) used 5 times in 7 day window\n",
    "Patagonia (Monahan and Ramage 2010) and Yukon (Semmens et al 2013?) used 3 times in 5 day window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Matias Fall 2022 forward facing indexer to assign rolling sum value to the beginning of window\n",
    "# Setting the window_size sets the number of observations,'14' would be 7 days (2 measurements per day)\n",
    "# If you don't want to use the forward facing indexer, then change \"window\" to a numeral, this will assign\n",
    "# the rolling sum value to the end of the window\n",
    "MOD_window = 10\n",
    "MOD_count = 1\n",
    "indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=MOD_window)\n",
    "window = indexer   # this assigns the rolling sum value to the beginning of the window\n",
    "#window = MOD_window # this assigns the rolling sum value to the end of the window\n",
    "\n",
    "# Number of Tb/DAV exceedances to trigger EHD = end of high DAV\n",
    "# At the current time, EHD is not using the forward facing indexer, using default behavior\n",
    "# From Matias Fall 2022\n",
    "# Theoretically you could do the same thing with the EDH_window here, but I'm not sure\n",
    "# you would ever want that\n",
    "EHD_window = 20 \n",
    "EHD_count = 7 \n",
    "\n",
    "# number of Tb/DAV exceedances to trigger MOD\n",
    "#From Johnson et al 2020 AMSRE rSIR Tb >= 249 DAV>=13 and AMSRE GRD Tb>=243 DAV>=14\n",
    "#From Johnson et al 2020 SSMI rSIR and GRD Tb>=247 DAV>=10\n",
    "DAV_threshold = 8\n",
    "Tb_threshold = 252"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate histogram - SIR - all data\n",
    "# This histogram will include all points that are in the CETB_SIR array\n",
    "year=2021\n",
    "\n",
    "#data = CETB_SIR[cal_year==year]\n",
    "data = data_SIR['TB'][data_SIR['cal_year']==year, :, :] # SIR data for all pixels in this year\n",
    "data = data[data>=0]\n",
    "bins = range(150, 300)  # bins for histogram\n",
    "fig,ax = plt.subplots()\n",
    "ax.hist(data, bins)\n",
    "ax.set_title('TB SIR Histogram, area=%s, year=%s' % (Site, str(year)))\n",
    "ax.axvline(x=Tb_threshold, color='red')\n",
    "ax.set_xlabel('Brightness Temp (K)') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data frames of MOD and EHD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## MOD of the GRD pixel - avg all years\n",
    "MOD_DOY_GRD_df, meltflag_GRD_df, EHD_DOY_GRD_df, EHDflag_GRD_df = MOD_array(\n",
    "    datadir_GRD, prefix_GRD, data_GRD, DAV_GRD, rows_cols_GRD, \n",
    "    subYears, window, MOD_count, EHD_window, EHD_count, DAV_threshold, Tb_threshold)\n",
    "MOD_DOY_GRD_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the meltflag dataframe \n",
    "\n",
    "Has an entry for each date (morning and evening) on rows, and for each pixel on columns \n",
    "\n",
    "It contains a 1 for any location/date that the melt criteria were met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meltflag_GRD_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sir MOD array - MOD will be in day of year (DOY) #changeback to Years for all years\n",
    "MOD_DOY_df, meltflag_df, EHD_DOY_df, EHDflag_df = MOD_array(\n",
    "    datadir_SIR, prefix_SIR, data_SIR, DAV_SIR, rows_cols_env, \n",
    "    subYears, window, MOD_count, EHD_window, EHD_count, DAV_threshold, Tb_threshold)\n",
    "MOD_DOY_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a histogram of the MODs for a selected year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "MOD_DOY_GRD_df.hist(ax=ax,column=2021)\n",
    "ax.set_title('MOD Histogram, area=%s, year=%s' % (Site, str(year)))\n",
    "ax.set_xlabel('DOY') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes about changes from original notebooks:\n",
    "\n",
    "The old notebooks used to call MOD_array to get the average MOD for a set of years \n",
    "and then call MOD_array_year for a given year of interest.\n",
    "\n",
    "Now we just call MOD_array for SIR data and GRD data, and get back a data frame with \n",
    "MOD columns for each individual year, and one column for the avg MOD for all the years.\n",
    "\n",
    "This runs much faster, and can be saved and just re-read from a saved file on disk.\n",
    "\n",
    "The old notebooks only calculated MOD, the current versions of the MOD_array function also calculate EHD.\n",
    "\n",
    "You might decide to change MOD_array to only do one or the other based on an input switch if you don't always\n",
    "want both calculations.\n",
    "\n",
    "Now we are setting up to save these data and the way to read them in another notebook is:\n",
    "\n",
    "new = pd.read_pickle(MOD_DOY_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the MOD by year data frames for SIR and GRD to pickle files\n",
    "\n",
    "Also saving geolocation and melt onset flag data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "\n",
    "# Set a short string in the filename to indicate whether window was the indexer function or a plain integer\n",
    "if isinstance(window, int):\n",
    "    rollingSumLabel = 'end'\n",
    "else:\n",
    "    rollingSumLabel = 'beg'\n",
    "    \n",
    "MODinfo = \"MOD_C%1dW%02d%sT%03dD%02d\" % (\n",
    "    MOD_count, MOD_window, rollingSumLabel, Tb_threshold, DAV_threshold)  \n",
    "meltflaginfo = \"meltflag_C%1dW%02d%sT%03dD%02d\" % (\n",
    "    MOD_count, MOD_window, rollingSumLabel, Tb_threshold, DAV_threshold) \n",
    "\n",
    "sirMODBasename = \"%s/%s.%s.%s.%s.%s.%s-%s.%s\" % (\n",
    "    outDir, data_SIR['gpd'], region, SiteLabel, sat_SIR, channel_SIR, \n",
    "    subYears[0], subYears[-1], MODinfo)\n",
    "grdMODBasename = \"%s/%s.%s.%s.%s.%s.%s-%s.%s\" % (\n",
    "    outDir, data_GRD['gpd'], region, SiteLabel, sat_GRD, channel_GRD, \n",
    "    subYears[0], subYears[-1], MODinfo)\n",
    "\n",
    "sirmeltflagBasename = \"%s/%s.%s.%s.%s.%s.%s-%s.%s\" % (\n",
    "    outDir, data_SIR['gpd'], region, SiteLabel, sat_SIR, channel_SIR, \n",
    "    subYears[0], subYears[-1], meltflaginfo)\n",
    "grdmeltflagBasename = \"%s/%s.%s.%s.%s.%s.%s-%s.%s\" % (\n",
    "    outDir, data_GRD['gpd'], region, SiteLabel, sat_GRD, channel_GRD, \n",
    "    subYears[0], subYears[-1], meltflaginfo)\n",
    "\n",
    "filename = \"%s.pkl\" % (sirMODBasename)\n",
    "MOD_DOY_df.to_pickle(filename)\n",
    "print(\"MOD_DOY dataframe saved to %s\\n\" % filename)\n",
    "\n",
    "filename = \"%s.pkl\" % (grdMODBasename)\n",
    "MOD_DOY_GRD_df.to_pickle(filename)\n",
    "print(\"MOD_DOY_GRD dataframe saved to %s\\n\" % filename)\n",
    "\n",
    "filename = \"%s.pkl\" % (sirmeltflagBasename)\n",
    "meltflag_df.to_pickle(filename)\n",
    "print(\"meltflag_df dataframe saved to %s\\n\" % filename)\n",
    "\n",
    "filename = \"%s.pkl\" % (grdmeltflagBasename)\n",
    "meltflag_GRD_df.to_pickle(filename)\n",
    "print(\"meltflag_GRD_df dataframe saved to %s\\n\" % filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the EHD by year data frames for SIR and GRD to pickle files\n",
    "\n",
    "Also saving geolocation and EHD flag data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(outDir):\n",
    "    os.makedirs(outDir)\n",
    "\n",
    "# Set a short string in the filename to indicate whether window was the indexer function or a plain integer\n",
    "if isinstance(EHD_window, int):\n",
    "    rollingSumLabel = 'end'\n",
    "else:\n",
    "    rollingSumLabel = 'beg'\n",
    "    \n",
    "EHDinfo = \"EHD_C%1dW%02d%sT%03dD%02d\" % (\n",
    "    EHD_count, EHD_window, rollingSumLabel, Tb_threshold, DAV_threshold)  \n",
    "EHDflaginfo = \"EHDflag_C%1dW%02d%sT%03dD%02d\" % (\n",
    "    EHD_count, EHD_window, rollingSumLabel, Tb_threshold, DAV_threshold) \n",
    "\n",
    "sirEHDBasename = \"%s/%s.%s.%s.%s.%s.%s-%s.%s\" % (\n",
    "    outDir, data_SIR['gpd'], region, SiteLabel, sat_SIR, channel_SIR, \n",
    "    subYears[0], subYears[-1], EHDinfo)\n",
    "grdEHDBasename = \"%s/%s.%s.%s.%s.%s.%s-%s.%s\" % (\n",
    "    outDir, data_GRD['gpd'], region, SiteLabel, sat_GRD, channel_GRD, \n",
    "    subYears[0], subYears[-1], EHDinfo)\n",
    "\n",
    "sirEHDflagBasename = \"%s/%s.%s.%s.%s.%s.%s-%s.%s\" % (\n",
    "    outDir, data_SIR['gpd'], region, SiteLabel, sat_SIR, channel_SIR, \n",
    "    subYears[0], subYears[-1], EHDflaginfo)\n",
    "grdEHDflagBasename = \"%s/%s.%s.%s.%s.%s.%s-%s.%s\" % (\n",
    "    outDir, data_GRD['gpd'], region, SiteLabel, sat_GRD, channel_GRD, \n",
    "    subYears[0], subYears[-1], EHDflaginfo)\n",
    "\n",
    "filename = \"%s.pkl\" % (sirEHDBasename)\n",
    "EHD_DOY_df.to_pickle(filename)\n",
    "print(\"EHD_DOY dataframe saved to %s\\n\" % filename)\n",
    "\n",
    "filename = \"%s.pkl\" % (grdEHDBasename)\n",
    "EHD_DOY_GRD_df.to_pickle(filename)\n",
    "print(\"EHD_DOY_GRD dataframe saved to %s\\n\" % filename)\n",
    "\n",
    "filename = \"%s.pkl\" % (sirEHDflagBasename)\n",
    "EHDflag_df.to_pickle(filename)\n",
    "print(\"EHDflag_df dataframe saved to %s\\n\" % filename)\n",
    "\n",
    "filename = \"%s.pkl\" % (grdEHDflagBasename)\n",
    "EHDflag_GRD_df.to_pickle(filename)\n",
    "print(\"EHDflag_GRD_df dataframe saved to %s\\n\" % filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optionally, save the MOD for each year as a geotiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the subYears date range out of the basename strings, so that the geotiff routine can add the year itself\n",
    "sirMODBasename = \"%s/%s.%s.%s.%s.%s.%s\" % (\n",
    "    outDir, data_SIR['gpd'], region, SiteLabel, sat_SIR, channel_SIR, MODinfo)\n",
    "grdMODBasename = \"%s/%s.%s.%s.%s.%s.%s\" % (\n",
    "    outDir, data_GRD['gpd'], region, SiteLabel, sat_GRD, channel_GRD, MODinfo)\n",
    "\n",
    "outSIR = write_df_to_geotiff(MOD_DOY_df, data_SIR['gpd'], sirMODBasename, verbose=True)\n",
    "outGRD = write_df_to_geotiff(MOD_DOY_GRD_df, data_GRD['gpd'], grdMODBasename, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optionally, save the EHD for each year as a geotiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the subYears date range out of the basename strings, so that the geotiff routine can add the year itself\n",
    "sirEHDBasename = \"%s/%s.%s.%s.%s.%s.%s\" % (\n",
    "    outDir, data_SIR['gpd'], region, SiteLabel, sat_SIR, channel_SIR, EHDinfo)\n",
    "grdEHDBasename = \"%s/%s.%s.%s.%s.%s.%s\" % (\n",
    "    outDir, data_GRD['gpd'], region, SiteLabel, sat_GRD, channel_GRD, EHDinfo)\n",
    "\n",
    "outSIR = write_df_to_geotiff(EHD_DOY_df, data_SIR['gpd'], sirEHDBasename, verbose=True)\n",
    "outGRD = write_df_to_geotiff(EHD_DOY_GRD_df, data_GRD['gpd'], grdEHDBasename, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Other things to potentially save here\n",
    "\n",
    "#Examples include: saving the DAV, or saving the std dev\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
